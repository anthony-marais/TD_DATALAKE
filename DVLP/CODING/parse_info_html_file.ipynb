{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import platform\n",
    "import getpass\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_ID</th>\n",
       "      <th>File_Path_Origin</th>\n",
       "      <th>File_Size</th>\n",
       "      <th>File_Date</th>\n",
       "      <th>File_Path_Destination</th>\n",
       "      <th>File_Destination_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>320.6 KB</td>\n",
       "      <td>2023-03-22 12:15:40.047528</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>320.6 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>223.4 KB</td>\n",
       "      <td>2023-03-22 12:15:40.051432</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>223.4 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>550.1 KB</td>\n",
       "      <td>2023-03-22 12:15:40.054262</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>550.1 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>196.9 KB</td>\n",
       "      <td>2023-03-22 12:15:40.055507</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>196.9 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>326.9 KB</td>\n",
       "      <td>2023-03-22 12:15:40.056867</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>326.9 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>599</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>484.0 KB</td>\n",
       "      <td>2023-03-22 12:15:40.737365</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>484.0 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>600</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>233.2 KB</td>\n",
       "      <td>2023-03-22 12:15:40.738420</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>233.2 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>601</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>202.5 KB</td>\n",
       "      <td>2023-03-22 12:15:40.739565</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>202.5 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>602</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>195.6 KB</td>\n",
       "      <td>2023-03-22 12:15:40.740496</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>195.6 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>603</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>217.7 KB</td>\n",
       "      <td>2023-03-22 12:15:40.741394</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>217.7 KB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_ID                                   File_Path_Origin File_Size  \\\n",
       "1          1  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  320.6 KB   \n",
       "2          2  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  223.4 KB   \n",
       "3          3  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  550.1 KB   \n",
       "4          4  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  196.9 KB   \n",
       "5          5  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  326.9 KB   \n",
       "..       ...                                                ...       ...   \n",
       "599      599  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  484.0 KB   \n",
       "600      600  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  233.2 KB   \n",
       "601      601  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  202.5 KB   \n",
       "602      602  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  195.6 KB   \n",
       "603      603  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  217.7 KB   \n",
       "\n",
       "                     File_Date  \\\n",
       "1   2023-03-22 12:15:40.047528   \n",
       "2   2023-03-22 12:15:40.051432   \n",
       "3   2023-03-22 12:15:40.054262   \n",
       "4   2023-03-22 12:15:40.055507   \n",
       "5   2023-03-22 12:15:40.056867   \n",
       "..                         ...   \n",
       "599 2023-03-22 12:15:40.737365   \n",
       "600 2023-03-22 12:15:40.738420   \n",
       "601 2023-03-22 12:15:40.739565   \n",
       "602 2023-03-22 12:15:40.740496   \n",
       "603 2023-03-22 12:15:40.741394   \n",
       "\n",
       "                                 File_Path_Destination File_Destination_Size  \n",
       "1    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              320.6 KB  \n",
       "2    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              223.4 KB  \n",
       "3    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              550.1 KB  \n",
       "4    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              196.9 KB  \n",
       "5    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              326.9 KB  \n",
       "..                                                 ...                   ...  \n",
       "599  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              484.0 KB  \n",
       "600  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              233.2 KB  \n",
       "601  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              202.5 KB  \n",
       "602  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              195.6 KB  \n",
       "603  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              217.7 KB  \n",
       "\n",
       "[603 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from python_function_datalake import MoveFileToLandingZone, GetOS,GenerateOSPathProject\n",
    "import click\n",
    "\n",
    "os_system = GetOS()\n",
    "path_project = GenerateOSPathProject(os_system)\n",
    "source_path = path_project[0]\n",
    "logfiles_path = path_project[1]\n",
    "landing_zone_linkedin_emp_path = path_project[2]\n",
    "landing_zone_glassdoor_soc_path = path_project[3]\n",
    "landing_zone_glassdoor_avi_path = path_project[4]\n",
    "linkedin_contains = path_project[5]\n",
    "glassdoor_soc_contains = path_project[6]\n",
    "glassdoor_avi_contains = path_project[7]\n",
    "path = path_project[8]\n",
    "endswith = path_project[9]\n",
    "contains_1 = path_project[10]\n",
    "contains_2 = path_project[11]\n",
    "delimiter_path = path_project[12]\n",
    "\n",
    "\n",
    "#print(source_path)\n",
    "#print(logfiles_path)\n",
    "#print(landing_zone_linkedin_emp_path)\n",
    "#print(landing_zone_glassdoor_soc_path)\n",
    "#print(landing_zone_glassdoor_avi_path)\n",
    "#print(linkedin_contains)\n",
    "#print(glassdoor_soc_contains)\n",
    "#print(glassdoor_avi_contains)\n",
    "#print(path)\n",
    "#print(endswith)\n",
    "#print(contains_1)\n",
    "#print(contains_2)\n",
    "#print(delimiter_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MoveFileToLandingZone(source_path, endswith, linkedin_contains, glassdoor_soc_contains, glassdoor_avi_contains,\n",
    "                      landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path, logfiles_path, delimiter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metadata_files = \"/home/virus/Documents/TD_DATALAKE/LOGFILES/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadMetadataFiles(path_metadata_files, file_name, delimiter_path):\n",
    "    \"\"\"\n",
    "    Load metadata files\n",
    "    \"\"\"\n",
    "    metadata = pd.read_csv(path_metadata_files+delimiter_path+file_name)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...\n",
       "1      /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...\n",
       "2      /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...\n",
       "3      /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...\n",
       "4      /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...\n",
       "                             ...                        \n",
       "598    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...\n",
       "599    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...\n",
       "600    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...\n",
       "601    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...\n",
       "602    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...\n",
       "Name: File_Path_Destination, Length: 603, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = LoadMetadataFiles(path_metadata_files,\"metadata-technical.csv\", delimiter_path)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(path_metadata_files, file_name, delimiter_path):\n",
    "    ''' Function to get data from html file where the file name contains a specific string (INFO) AND (LINKEDIN) '''\n",
    "\n",
    "    \"\"\"\n",
    "    Call LoadMetadataFiles function\n",
    "    \"\"\"\n",
    "    landing_zone_path = LoadMetadataFiles(path_metadata_files, file_name, delimiter_path)\n",
    "    \n",
    "    linkedin_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\"LINKEDIN\")]\n",
    "    len_linkedin_files = len(linkedin_files)\n",
    "    glassdoor_avis_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\"AVIS-SOC\")]\n",
    "    len_glassdoor_avis_files = len(glassdoor_avis_files)\n",
    "    glassdoor_soc_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\"INFO-SOC\")]\n",
    "    len_glassdoor_soc_files = len(glassdoor_soc_files)\n",
    "    \n",
    "    print(\"Number of Linkedin files: \", len_linkedin_files)\n",
    "    print(\"Number of Glassdoor avis files: \", len_glassdoor_avis_files)\n",
    "    print(\"Number of Glassdoor soc files: \", len_glassdoor_soc_files)\n",
    "    list_of_data = []\n",
    "    title_file = []\n",
    "    columns_dataframe = [\"file\",\"title\",\"society\",\"city\",\"country\"]\n",
    "    data_title = []\n",
    "    data_society = []\n",
    "    data_city = []\n",
    "    data_country = []\n",
    "\n",
    "\n",
    "    for i in linkedin_files[\"File_Path_Destination\"]:\n",
    "        title_file.append(i)\n",
    "\n",
    "        \n",
    "        with open(i, 'r', encoding=\"utf-8\",errors=\"replace\") as f:\n",
    "            soup = bs(f, 'html.parser')\n",
    "            title = [i for i in soup.find_all('h1', attrs = {'class':'topcard__title'})]\n",
    "            title_text = title[0].text\n",
    "            society = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor'})]\n",
    "            society_text = society[0].text\n",
    "            city = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor topcard__flavor--bullet'})]\n",
    "            \n",
    "            \n",
    "            city_text = city[0].text\n",
    "            country_text = city_text.split(\",\")[1].strip()\n",
    "            print(city_text.split(\",\"))\n",
    "            \n",
    "            \n",
    "        #    city_text = city_text.split(\",\")[0]\n",
    "        #    data_title.append(title_text)\n",
    "        #    data_society.append(society_text)\n",
    "        #    data_city.append(city_text)\n",
    "        #    data_country.append(country_text)\n",
    "\n",
    "\n",
    "#    data_linkedin = pd.DataFrame(list(zip(title_file,data_title,data_society,data_city,data_country)),columns=columns_dataframe)\n",
    "    \n",
    "        \n",
    "#    return data_linkedin\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Linkedin files:  254\n",
      "Number of Glassdoor avis files:  209\n",
      "Number of Glassdoor soc files:  140\n",
      "['Sophia Antipolis', ' FR']\n",
      "['Ã‰vry', ' ÃŽle-de-France', ' France']\n",
      "['Puteaux', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Neuilly-sur-Seine', ' ÃŽle-de-France', ' France']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Lyon', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Puteaux', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Roubaix', ' Nord-Pas-de-Calais', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Lyon', ' Auvergne-RhÃ´ne-Alpes', ' France']\n",
      "['Paris', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['75009', ' Paris', ' Ile-de-France', ' France']\n",
      "['Paris', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Talence', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Roubaix', ' Nord-Pas-de-Calais', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Bordeaux', ' Nouvelle-Aquitaine', ' France']\n",
      "['Le Mans', ' FR']\n",
      "['Aix-en-Provence', ' FR']\n",
      "['Bezons', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Nantes', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Saint-Maurice-sur-Dargoire', ' FR']\n",
      "['Saint-Herblain', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' Auvergne-RhÃ´ne-Alpes', ' France']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' Auvergne-RhÃ´ne-Alpes', ' France']\n",
      "['Neuilly-sur-Seine', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Paris 02', ' ÃŽle-de-France', ' France']\n",
      "['Paris Area', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Montpellier', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Aix-en-Provence', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Dargoire', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Paris Area', ' France']\n",
      "['Lille', ' Nord-Pas-de-Calais', ' France']\n",
      "['Paris 16', ' ÃŽle-de-France', ' France']\n",
      "['Ã‰cully', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Lyon 01', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Paris', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Saint-Maurice-sur-Dargoire', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Paris 04', ' ÃŽle-de-France', ' France']\n",
      "['Puteaux', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Villeurbanne', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Lille', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Paris', ' Ile-de-France', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Gradignan', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon Area', ' France']\n",
      "['Lyon', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Massy', ' Ile-de-France', ' France']\n",
      "['Paris', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Villeurbanne', ' FR']\n",
      "['Villeurbanne', ' FR']\n",
      "['Lyon', ' Auvergne-Rhone-Alpes', ' France']\n",
      "['Nice', \" Provence-Alpes-Cote d'Azur\", ' France']\n",
      "['Paris', ' FR']\n",
      "['Paris Area', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Colombes', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Montrouge', ' ÃŽle-de-France', ' France']\n",
      "['Azur', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Bron', ' FR']\n",
      "['Paris', ' FR']\n",
      "['69230', ' Saint-Genis-Laval', ' Auvergne-Rhone-Alpes', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Nantes', ' FR']\n",
      "['Paris Area', ' France']\n",
      "['Viroflay', ' ÃŽle-de-France', ' France']\n",
      "['Limonest', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Courbevoie', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['RÃ©gion de Lyon', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Aix-en-Provence', ' FR']\n",
      "['Lyon 06', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Rennes', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Fontenay-sous-Bois', ' FR']\n",
      "['Paris', ' ÃŽle-de-France', ' France']\n",
      "['Lyon', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Marseille', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Clichy', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Neuilly-sur-Seine', ' ÃŽle-de-France', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Paris', ' Ile-de-France', ' France']\n",
      "['Paris', ' FR']\n",
      "['Ã‰cully', ' FR']\n",
      "['Neuilly-sur-Seine', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Cestas', ' Nouvelle-Aquitaine', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Malakoff', ' FR']\n",
      "['Lille Area', ' France']\n",
      "['Aix-en-Provence', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Aix-en-Provence', ' Provence-Alpes-CÃ´te dâ€™Azur', ' France']\n",
      "['Courbevoie', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Strasbourg', ' Alsace', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Montpellier', ' FR']\n",
      "['Nantes', ' FR']\n",
      "['MÃ©rignac', ' FR']\n",
      "['Lyon Area', ' France']\n",
      "['Lille', ' FR']\n",
      "['Departement des Hauts-de-Seine', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Paris', ' ÃŽle-de-France', ' France']\n",
      "['Paris', ' FR']\n",
      "['Lille', ' FR']\n",
      "['Lille', ' FR']\n",
      "['Paris', ' FR']\n",
      "['RÃ©gion de Lyon', ' France']\n",
      "['Nice', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Limonest', ' FR']\n",
      "['Montpellier', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Boulogne-Billancourt', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Sainte-Julie', ' FR']\n",
      "['Montrouge', ' ÃŽle-de-France', ' France']\n",
      "['Paris', ' Ile-de-France', ' France']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Aix-en-Provence', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Ã‰cully', ' FR']\n",
      "['Bordeaux', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Paris', ' FR']\n",
      "['Neuville-en-Ferrain', ' Hauts-de-France', ' France']\n",
      "['Paris', ' ÃŽle-de-France', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Montpellier', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Levallois-Perret', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Limonest', ' FR']\n",
      "['RÃ©gion de Paris', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Niort', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Paris La DÃ©fense', ' ÃŽle-de-France', ' France']\n",
      "['Aix-en-Provence', ' FR']\n",
      "['Toulouse', ' FR']\n",
      "['Lyon', ' Auvergne-RhÃ´ne-Alpes', ' France']\n",
      "['Montpellier', ' FR']\n",
      "['La Ciotat', ' FR']\n",
      "['MÃ©rignac', ' FR']\n",
      "['Paris', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Le Plessis-Belleville', ' Picardie', ' France']\n",
      "['Lyon', ' FR']\n",
      "['Boulogne-Billancourt', ' ÃŽle-de-France', ' France']\n",
      "['Bezons', ' ÃŽle-de-France', ' France']\n",
      "['Saint-Priest', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Lyon', ' FR']\n",
      "['Paris', ' ÃŽle-de-France', ' France']\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m GetData(path_metadata_files,\u001b[39m\"\u001b[39;49m\u001b[39mmetadata-technical.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, delimiter_path)\n",
      "Cell \u001b[0;32mIn[99], line 42\u001b[0m, in \u001b[0;36mGetData\u001b[0;34m(path_metadata_files, file_name, delimiter_path)\u001b[0m\n\u001b[1;32m     38\u001b[0m city \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m'\u001b[39m, attrs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mtopcard__flavor topcard__flavor--bullet\u001b[39m\u001b[39m'\u001b[39m})]\n\u001b[1;32m     41\u001b[0m city_text \u001b[39m=\u001b[39m city[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n\u001b[0;32m---> 42\u001b[0m country_text \u001b[39m=\u001b[39m city_text\u001b[39m.\u001b[39;49msplit(\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     43\u001b[0m \u001b[39mprint\u001b[39m(city_text\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "GetData(path_metadata_files,\"metadata-technical.csv\", delimiter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sophia Antipolis\n",
      "FR\n"
     ]
    }
   ],
   "source": [
    "text = \"Sophia Antipolis, FR\"\n",
    "\n",
    "city = text.split(\",\")[0]\n",
    "country = text.split(\",\")[1].strip()\n",
    "\n",
    "print(city)\n",
    "print(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linux'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetOS():\n",
    "    system = platform.system()\n",
    "    return system\n",
    "\n",
    "os_system = GetOS()\n",
    "os_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linux'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = platform.system()\n",
    "system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUser():\n",
    "    user = getpass.getuser()\n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateOSPathProject(os_system):\n",
    "\n",
    "    load_dotenv()\n",
    "    \n",
    "    if os_system != \"Windows\":\n",
    "        user = GetUser()\n",
    "        source_path = os.environ.get('source_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        logfiles_path = os.environ.get('logfiles_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        landing_zone_linkedin_emp_path = os.environ.get(\n",
    "            'landing_zone_linkedin_emp_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        landing_zone_glassdoor_soc_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_soc_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        landing_zone_glassdoor_avi_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_avi_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        linkedin_contains = os.environ.get('linkedin_contains')\n",
    "        glassdoor_soc_contains = os.environ.get('glassdoor_soc_contains')\n",
    "        glassdoor_avi_contains = os.environ.get('glassdoor_avi_contains')\n",
    "        path = os.environ.get('path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        endswith = os.environ.get('endswith')\n",
    "        contains_1 = os.environ.get('contains_1')\n",
    "        contains_2 = os.environ.get('contains_2')\n",
    "        delimiter_path = \"/\"\n",
    "\n",
    "    elif os_system == \"Windows\":\n",
    "        source_path = os.environ.get('source_path_windows')\n",
    "        logfiles_path = os.environ.get('logfiles_path_windows')\n",
    "        landing_zone_linkedin_emp_path = os.environ.get(\n",
    "            'landing_zone_linkedin_emp_path_windows')\n",
    "        landing_zone_glassdoor_soc_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_soc_path_windows')\n",
    "        landing_zone_glassdoor_avi_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_avi_path_windows')\n",
    "        linkedin_contains = os.environ.get('linkedin_contains')\n",
    "        glassdoor_soc_contains = os.environ.get('glassdoor_soc_contains')\n",
    "        glassdoor_avi_contains = os.environ.get('glassdoor_avi_contains')\n",
    "        path = os.environ.get('path_windows')\n",
    "        endswith = os.environ.get('endswith')\n",
    "        contains_1 = os.environ.get('contains_1')\n",
    "        contains_2 = os.environ.get('contains_2')\n",
    "        delimiter_path = \"\\\\\"\n",
    "\n",
    "\n",
    "    return source_path, logfiles_path, landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path, linkedin_contains, glassdoor_soc_contains, glassdoor_avi_contains, path, endswith, contains_1, contains_2, delimiter_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'source_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os_path_project \u001b[39m=\u001b[39m GenerateOSPathProject(os_system)\u001b[39m.\u001b[39;49msource_path\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'source_path'"
     ]
    }
   ],
   "source": [
    "os_path_project = GenerateOSPathProject(os_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'source_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m source_path\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_path' is not defined"
     ]
    }
   ],
   "source": [
    "source_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPath(os_path_project):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateWotkingPath(source_path,user):\n",
    "    working_path = source_path.replace(\"//\", \"/\"+user+\"/\")\n",
    "    return working_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenerateWotkingPath(source_path,user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUser():\n",
    "    user = getpass.getuser()\n",
    "    return user\n",
    "\n",
    "def GetOS():\n",
    "    system = platform.system()\n",
    "    if system == \"/\":\n",
    "        return \"Linux\"\n",
    "    elif system == \"Windows\":\n",
    "        return \"\\\\\"\n",
    "    else:\n",
    "        return \"/\"\n",
    "\n",
    "def DefinePathOS(path):\n",
    "    delimiter_path = GetOS()\n",
    "    if delimiter_path == \"Linux\":\n",
    "        return path.replace(\"\\\\\", \"/\")\n",
    "    elif delimiter_path == \"Windows\":\n",
    "        return path.replace(\"/\", \"\\\\\")\n",
    "    else:\n",
    "        return path.replace(\"\\\\\", \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetOS():\n",
    "    system = platform.system()\n",
    "    if system == \"/\":\n",
    "        return \"Linux\"\n",
    "    elif system == \"Windows\":\n",
    "        return \"\\\\\"\n",
    "    else:\n",
    "        return \"/\"\n",
    "\n",
    "''' Function to count number of file in a directory '''\n",
    "def GetFilesPath(path,endswith,delimiter_path):\n",
    "    count = 0\n",
    "    list_of_files = []\n",
    "    ### open Files \n",
    "    for file in os.listdir(path):\n",
    "        count += 1\n",
    "    \n",
    "    list_dir_path = [x for x in os.listdir(path) if x.endswith(endswith)]    \n",
    "    \n",
    "    for Files in range(count):\n",
    "        list_of_files.append(path+delimiter_path+list_dir_path[Files])\n",
    "\n",
    "\n",
    "    return list_of_files\n",
    "\n",
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "\n",
    "\n",
    "def file_size(file_path):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \"\"\"\n",
    "    list_file_size = []\n",
    "    for i in range(len(file_path)):\n",
    "        if os.path.isfile(file_path[i]):\n",
    "            file_info = os.stat(file_path[i])\n",
    "\n",
    "            list_file_size.append(convert_bytes(file_info.st_size))\n",
    "\n",
    "    return list_file_size\n",
    "\n",
    "\n",
    "\n",
    "def MakeMetadataFile(file_path,file_size):\n",
    "    columns_list = [\"File_ID\",\"File_Path_Origin\",\"File_Size\",\"File_Date\"]\n",
    "    df = pd.DataFrame(columns = columns_list)\n",
    "    for i in range(len(file_path)):\n",
    "\n",
    "        df.loc[i] = [i+1,file_path[i],file_size[i],datetime.now()]\n",
    "\n",
    "    df.index = df.index + 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def MoveFileToLandingZone(source_path, endswith,linkedin_contains,glassdoor_soc_contains, glassdoor_avi_contains, landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path,logfiles_path,delimiter_path):\n",
    "\n",
    "    file_path = GetFilesPath(source_path,endswith,delimiter_path)\n",
    "    files_sizes = file_size(file_path)\n",
    "    general_metadata = MakeMetadataFile(file_path, files_sizes)\n",
    "    file_destination = []\n",
    "    file_destination_size = []\n",
    "\n",
    "    for file in general_metadata.File_Path_Origin:\n",
    "        if linkedin_contains in file:\n",
    "\n",
    "            file_destination.append(landing_zone_linkedin_emp_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            \n",
    "            shutil.copy(file, landing_zone_linkedin_emp_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            \n",
    "            file_destination_size.append(file_size([landing_zone_linkedin_emp_path +\n",
    "                        file.split(delimiter_path)[-1]]))\n",
    "\n",
    "        elif glassdoor_soc_contains in file:\n",
    "\n",
    "            file_destination.append(landing_zone_glassdoor_soc_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "\n",
    "\n",
    "            shutil.copy(file, landing_zone_glassdoor_soc_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            \n",
    "            file_destination_size.append(file_size([landing_zone_glassdoor_soc_path +\n",
    "                        file.split(delimiter_path)[-1]]))\n",
    "            \n",
    "        elif glassdoor_avi_contains in file:\n",
    "\n",
    "            file_destination.append(landing_zone_glassdoor_avi_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "\n",
    "            shutil.copy(file, landing_zone_glassdoor_avi_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            file_destination_size.append(file_size([landing_zone_glassdoor_avi_path +\n",
    "                        file.split(delimiter_path)[-1]]))\n",
    "            \n",
    "\n",
    "    general_metadata[\"File_Path_Destination\"] = file_destination\n",
    "    general_metadata[\"File_Destination_Size\"] = file_destination_size\n",
    "\n",
    "    general_metadata[\"File_Destination_Size\"] = general_metadata[\"File_Destination_Size\"].apply(lambda x: x[0])\n",
    "\n",
    "    general_metadata.to_csv(logfiles_path+\"/metadata-technical.csv\",index=None,encoding=\"utf-8\",header=True)\n",
    "\n",
    "\n",
    "    return general_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB\"\n",
    "logfiles_path = \"/home/virus/Documents/TD_DATALAKE/LOGFILES\"\n",
    "landing_zone_linkedin_emp_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/LINKEDIN/EMP/\"\n",
    "landing_zone_glassdoor_soc_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/SOC/\"\n",
    "landing_zone_glassdoor_avi_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/AVI/\"\n",
    "linkedin_contains = \"LINKEDIN\"\n",
    "glassdoor_soc_contains = \"INFO-SOC\"\n",
    "glassdoor_avi_contains = \"AVIS-SOC\"\n",
    "path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB\"\n",
    "endswith = \".html\"\n",
    "contains_1 = \"INFO\"\n",
    "contains_2 = \"LINKEDIN\"\n",
    "delimiter_path = GetOS()\n",
    "name_file = \"metadata-technical.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = MoveFileToLandingZone(source_path, endswith, linkedin_contains, glassdoor_soc_contains, glassdoor_avi_contains,\n",
    "                                landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path, logfiles_path, delimiter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_ID</th>\n",
       "      <th>File_Path_Origin</th>\n",
       "      <th>File_Size</th>\n",
       "      <th>File_Date</th>\n",
       "      <th>File_Path_Destination</th>\n",
       "      <th>File_Destination_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>320.6 KB</td>\n",
       "      <td>2023-02-27 19:28:44.628131</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>320.6 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>223.4 KB</td>\n",
       "      <td>2023-02-27 19:28:44.630157</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>223.4 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>550.1 KB</td>\n",
       "      <td>2023-02-27 19:28:44.634159</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>550.1 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>196.9 KB</td>\n",
       "      <td>2023-02-27 19:28:44.637487</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>196.9 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>326.9 KB</td>\n",
       "      <td>2023-02-27 19:28:44.640354</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>326.9 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>599</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>484.0 KB</td>\n",
       "      <td>2023-02-27 19:28:45.360059</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>484.0 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>600</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>233.2 KB</td>\n",
       "      <td>2023-02-27 19:28:45.361070</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>233.2 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>601</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>202.5 KB</td>\n",
       "      <td>2023-02-27 19:28:45.362049</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>202.5 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>602</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>195.6 KB</td>\n",
       "      <td>2023-02-27 19:28:45.363078</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>195.6 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>603</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>217.7 KB</td>\n",
       "      <td>2023-02-27 19:28:45.364077</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>217.7 KB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_ID                                   File_Path_Origin File_Size  \\\n",
       "1          1  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  320.6 KB   \n",
       "2          2  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  223.4 KB   \n",
       "3          3  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  550.1 KB   \n",
       "4          4  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  196.9 KB   \n",
       "5          5  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  326.9 KB   \n",
       "..       ...                                                ...       ...   \n",
       "599      599  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  484.0 KB   \n",
       "600      600  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  233.2 KB   \n",
       "601      601  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  202.5 KB   \n",
       "602      602  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  195.6 KB   \n",
       "603      603  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  217.7 KB   \n",
       "\n",
       "                     File_Date  \\\n",
       "1   2023-02-27 19:28:44.628131   \n",
       "2   2023-02-27 19:28:44.630157   \n",
       "3   2023-02-27 19:28:44.634159   \n",
       "4   2023-02-27 19:28:44.637487   \n",
       "5   2023-02-27 19:28:44.640354   \n",
       "..                         ...   \n",
       "599 2023-02-27 19:28:45.360059   \n",
       "600 2023-02-27 19:28:45.361070   \n",
       "601 2023-02-27 19:28:45.362049   \n",
       "602 2023-02-27 19:28:45.363078   \n",
       "603 2023-02-27 19:28:45.364077   \n",
       "\n",
       "                                 File_Path_Destination File_Destination_Size  \n",
       "1    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              320.6 KB  \n",
       "2    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              223.4 KB  \n",
       "3    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              550.1 KB  \n",
       "4    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              196.9 KB  \n",
       "5    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              326.9 KB  \n",
       "..                                                 ...                   ...  \n",
       "599  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              484.0 KB  \n",
       "600  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              233.2 KB  \n",
       "601  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              202.5 KB  \n",
       "602  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              195.6 KB  \n",
       "603  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              217.7 KB  \n",
       "\n",
       "[603 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadMetadataFiles(path,delimiter_path,name_file):\n",
    "    data = pd.read_csv(path+delimiter_path+name_file,encoding=\"utf-8\")\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB\"\n",
    "landing_zone_linkedin_emp_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/LINKEDIN/EMP/\"\n",
    "file_path = GetFilesPath(source_path)\n",
    "files_sizes = file_size(file_path)\n",
    "\n",
    "linkedin_contains = \"LINKEDIN\"\n",
    "\n",
    "\n",
    "\n",
    "df = MakeMetadataFile(file_path,files_sizes)\n",
    "\n",
    "\n",
    "\n",
    "''' Function moove linkedin file to landing zone '''\n",
    "def MoveFileToLandingZone(source_path,linkedin_contains,landing_zone_linkedin_emp_path):\n",
    "    \n",
    "    #list_data = []\n",
    "    columns_dataframe = [\"title\",\"society\",\"city\"]\n",
    "    data_title = []\n",
    "    data_society = []\n",
    "    data_city = []\n",
    "    \n",
    "    file_path = GetFilesPath(source_path)\n",
    "    files_sizes = file_size(file_path)\n",
    "    df = MakeMetadataFile(file_path,files_sizes)\n",
    "\n",
    "\n",
    "    for file in df.File_Name:\n",
    "        if linkedin_contains in file:\n",
    "            \n",
    "            with open(file, 'r', encoding=\"utf-8\",errors=\"replace\") as f:\n",
    "                soup = bs(f, 'html.parser')\n",
    "                title = [i for i in soup.find_all('h1', attrs = {'class':'topcard__title'})]\n",
    "                society = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor'})]\n",
    "                city = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor topcard__flavor--bullet'})]\n",
    "                data_title.append(title[0].text)\n",
    "                data_society.append(society[0].text)\n",
    "                data_city.append(city[0].text)\n",
    "                \n",
    "                df_data = pd.DataFrame(columns = columns_dataframe)\n",
    "\n",
    "                df_data = pd.DataFrame(list(zip(data_title,data_society,data_city)),columns=columns_dataframe)\n",
    "\n",
    "            shutil.copy(file,landing_zone_linkedin_emp_path+file.split(\"/\")[-1])\n",
    "\n",
    "        \n",
    "    result = pd.concat([df,df_data],axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "testing = MoveFileToLandingZone(source_path,linkedin_contains,landing_zone_linkedin_emp_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_ID</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>File_Size</th>\n",
       "      <th>File_Date</th>\n",
       "      <th>title</th>\n",
       "      <th>society</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>320.6 KB</td>\n",
       "      <td>2023-02-23 19:57:43.167425</td>\n",
       "      <td>DÃ©veloppeur Talend Big Data H/F</td>\n",
       "      <td>Accor</td>\n",
       "      <td>Ã‰vry, ÃŽle-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>223.4 KB</td>\n",
       "      <td>2023-02-23 19:57:43.168142</td>\n",
       "      <td>Consultant / Consultante dÃ©cisionnel - Busines...</td>\n",
       "      <td>PERFECTSIGHT</td>\n",
       "      <td>Puteaux, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>550.1 KB</td>\n",
       "      <td>2023-02-23 19:57:43.169193</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Teads.tv</td>\n",
       "      <td>Paris, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>196.9 KB</td>\n",
       "      <td>2023-02-23 19:57:43.170375</td>\n",
       "      <td>CDI IngÃ©nieur de DÃ©veloppement / moteurs d'opt...</td>\n",
       "      <td>GROUPE M6</td>\n",
       "      <td>Neuilly-sur-Seine, ÃŽle-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>326.9 KB</td>\n",
       "      <td>2023-02-23 19:57:43.171607</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Awalee Consulting</td>\n",
       "      <td>RÃ©gion de Paris, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>600.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>233.2 KB</td>\n",
       "      <td>2023-02-23 19:57:43.795660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>601.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>202.5 KB</td>\n",
       "      <td>2023-02-23 19:57:43.796711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>602.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>195.6 KB</td>\n",
       "      <td>2023-02-23 19:57:43.797723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>603.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>217.7 KB</td>\n",
       "      <td>2023-02-23 19:57:43.798879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>INGENIEUR EN DEVELOPPEMENT BIG DATA H/F</td>\n",
       "      <td>Altran</td>\n",
       "      <td>Sophia Antipolis, FR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>604 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_ID                                          File_Name File_Size  \\\n",
       "1        1.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  320.6 KB   \n",
       "2        2.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  223.4 KB   \n",
       "3        3.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  550.1 KB   \n",
       "4        4.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  196.9 KB   \n",
       "5        5.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  326.9 KB   \n",
       "..       ...                                                ...       ...   \n",
       "600    600.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  233.2 KB   \n",
       "601    601.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  202.5 KB   \n",
       "602    602.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  195.6 KB   \n",
       "603    603.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  217.7 KB   \n",
       "0        NaN                                                NaN       NaN   \n",
       "\n",
       "                     File_Date  \\\n",
       "1   2023-02-23 19:57:43.167425   \n",
       "2   2023-02-23 19:57:43.168142   \n",
       "3   2023-02-23 19:57:43.169193   \n",
       "4   2023-02-23 19:57:43.170375   \n",
       "5   2023-02-23 19:57:43.171607   \n",
       "..                         ...   \n",
       "600 2023-02-23 19:57:43.795660   \n",
       "601 2023-02-23 19:57:43.796711   \n",
       "602 2023-02-23 19:57:43.797723   \n",
       "603 2023-02-23 19:57:43.798879   \n",
       "0                          NaT   \n",
       "\n",
       "                                                 title            society  \\\n",
       "1                      DÃ©veloppeur Talend Big Data H/F              Accor   \n",
       "2    Consultant / Consultante dÃ©cisionnel - Busines...       PERFECTSIGHT   \n",
       "3                                       Data Scientist           Teads.tv   \n",
       "4    CDI IngÃ©nieur de DÃ©veloppement / moteurs d'opt...          GROUPE M6   \n",
       "5                                       Data Scientist  Awalee Consulting   \n",
       "..                                                 ...                ...   \n",
       "600                                                NaN                NaN   \n",
       "601                                                NaN                NaN   \n",
       "602                                                NaN                NaN   \n",
       "603                                                NaN                NaN   \n",
       "0              INGENIEUR EN DEVELOPPEMENT BIG DATA H/F             Altran   \n",
       "\n",
       "                                         city  \n",
       "1                 Ã‰vry, ÃŽle-de-France, France  \n",
       "2                                 Puteaux, FR  \n",
       "3                                   Paris, FR  \n",
       "4    Neuilly-sur-Seine, ÃŽle-de-France, France  \n",
       "5                     RÃ©gion de Paris, France  \n",
       "..                                        ...  \n",
       "600                                       NaN  \n",
       "601                                       NaN  \n",
       "602                                       NaN  \n",
       "603                                       NaN  \n",
       "0                        Sophia Antipolis, FR  \n",
       "\n",
       "[604 rows x 7 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.pivot_table(index=[\"title\",\"society\",\"city\"],values=[\"File_Name\"],aggfunc=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['320.6 KB']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_size([\"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/SOC/13713-AVIS-SOC-GLASSDOOR-E3142186_P1.html\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1181346619.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[68], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    for i enumerate(list):\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "for i enumerate(list):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function to get data from html file where the file name contains a specific string (INFO) AND (LINKEDIN) '''\n",
    "def GetDataFromLinkedinFiles(opened_files):\n",
    "    list_of_data = []\n",
    "    columns_dataframe = [\"file\",\"title\",\"society\",\"city\"]\n",
    "    data_title = []\n",
    "    data_society = []\n",
    "    data_city = []\n",
    "    for i in range(len(opened_files)):\n",
    "        with open(opened_files[i], 'r', encoding=\"utf-8\",errors=\"replace\") as f:\n",
    "\n",
    "            soup = bs(f, 'html.parser')\n",
    "            title = [i for i in soup.find_all('h1', attrs = {'class':'topcard__title'})]\n",
    "            society = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor'})]\n",
    "            city = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor topcard__flavor--bullet'})]\n",
    "            data_title.append(title[0].text)\n",
    "            data_society.append(society[0].text)\n",
    "            data_city.append(city[0].text)\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(opened_files,data_title,data_society,data_city)),columns=columns_dataframe)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elif \"GLASSDOOR\" and \"AVI\" in i:\n",
    "        #    shutil.copy(i,\"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/AVI/\"+i.split(\"/\")[-1])\n",
    "        #elif \"SOC\" and \"GLASSDOOR\" in i:\n",
    "        #    shutil.copy(i,\"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/SOC/\"+i.split(\"/\")[-1])\n",
    "        #else:\n",
    "        #    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/virus/Documents/TD_DATALAKE/LOGFILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glassdoor_emp_path,glassdoor_soc_path ,linkedin_contains,glassdoor_contains,glassdoor_soc_contains_1,glassdoor_soc_contains_2):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "    for i in range(len(file_path)):\n",
    "        if linkedin_contains in file_path[i] and endswith in file_path[i]:\n",
    "            shutil.copy(file_path[i],landing_zone_linkedin_emp_path+\"/\"+i)\n",
    "\n",
    "\n",
    "\n",
    "        elif contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],landing_zone_glassdoor_emp_path+\"/\"+file_path[i].split(\"\\\\\")[-1])\n",
    "        \n",
    "        elif if contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],glassdoor_soc_path+\"\\\\\"+file_path[i].split(\"\\\\\")[-1])\n",
    "\n",
    "\n",
    "def MoveGlassdoorAvisToLandingZone(file_path,glassdoor_emp_path,contains_1,contains_2):\n",
    "    for i in range(len(file_path)):\n",
    "        if contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],glassdoor_emp_path+\"\\\\\"+file_path[i].split(\"\\\\\")[-1])\n",
    "\n",
    "def MoveGlassdoorSocToLandingZone(file_path,glassdoor_soc_path,contains_1,contains_2):\n",
    "    for i in range(len(file_path)):\n",
    "        if contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],glassdoor_soc_path+\"\\\\\"+file_path[i].split(\"\\\\\")[-1])\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#file_path = GetFilesPath(path)\n",
    "#print(file_path[0])\n",
    "#landing_zone_linkedin_emp_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/LINKEDIN/EMP\"\n",
    "#landing_zone_glassdoor_emp_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\AVI\"\n",
    "#landing_zone_glassdoor_soc_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\SOC\"\n",
    "\n",
    "#endswith = \".html\"\n",
    "#linkedin_contains = \"LINKEDIN\"\n",
    "#glassdoor_contains = \"GLASSDOR\"\n",
    "#glassdoor_soc_contains_1 = \"SOC\"\n",
    "#glassdoor_soc_contains_2 = \"AVIS\"\n",
    "\n",
    "#MoveFileToLandingZone(file_path,linkedin_emp_path,glassdoor_emp_path,glassdoor_soc_path ,linkedin_contains,glassdoor_contains,glassdoor_soc_contains_1,glassdoor_soc_contains_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MoveFileToLandingZone(file_path,landing_zone_linkedin_emp_path,endswith,linkedin_contains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13546-AVIS-SOC-GLASSDOOR-E12966_P1.html'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = GetFilesPath(path)\n",
    "\n",
    "file_path[0].split(\"\\\\\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m file_path \u001b[39m=\u001b[39m GetFilesPath(path)\n\u001b[0;32m      2\u001b[0m linkedin_emp_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mTD_DATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m1_LANDING_ZONE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mLINKEDIN\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mEMP\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m glassdoor_emp_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mTD_DATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m1_LANDING_ZONE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mGLASSDOOR\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mAVI\u001b[39m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[33], line 12\u001b[0m, in \u001b[0;36mGetFilesPath\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      9\u001b[0m list_dir_path \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(path) \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mendswith(endswith)]    \n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m Files \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(count):\n\u001b[1;32m---> 12\u001b[0m     list_of_files\u001b[39m.\u001b[39mappend(path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mlist_dir_path[Files])\n\u001b[0;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m list_of_files\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "file_path = GetFilesPath(path)\n",
    "linkedin_emp_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\LINKEDIN\\\\EMP\"\n",
    "glassdoor_emp_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\AVI\"\n",
    "glassdoor_soc_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\SOC\"\n",
    "\n",
    "endswith = \".html\"\n",
    "\n",
    "\n",
    "linkedin_contains = \"LINKEDIN\"\n",
    "glassdoor_contains = \"GLASSDOR\"\n",
    "glassdoor_soc_contains_1 = \"SOC\"\n",
    "glassdoor_soc_contains_2 = \"AVIS\"\n",
    "\n",
    "MoveLinkedinFileToLandingZone(file_path,linkedin_emp_path,linkedin_contains_1,linkedin_contains_2)\n",
    "\n",
    "MoveGlassdoorSocToLandingZone(file_path,glassdoor_emp_path,glassdoor_soc_contains_1,glassdoor_soc_contains_2)\n",
    "\n",
    "MoveGlassdoorAvisToLandingZone(file_path,glassdoor_soc_path,glassdoor_soc_contains_1,glassdoor_soc_contains_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function to count number of file in a directory '''\n",
    "def GetPathLinkedinFiles(path,endswith,info,linkedin):\n",
    "    count = 0\n",
    "    list_of_files = []\n",
    "    ### open Files \n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(endswith) and file.__contains__(info) and file.__contains__(linkedin):\n",
    "            count += 1\n",
    "    list_dir_path = [x for x in os.listdir(path) if x.endswith(endswith) and x.__contains__(info) and x.__contains__(linkedin)]    \n",
    "    for FileLinkedin in range(count):\n",
    "        list_of_files.append(path+\"\\\\\"+list_dir_path[FileLinkedin])\n",
    "\n",
    "    return list_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2 GB\n"
     ]
    }
   ],
   "source": [
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "\n",
    "\n",
    "def file_size(file_path):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \"\"\"\n",
    "    if os.path.isfile(file_path):\n",
    "        file_info = os.stat(file_path)\n",
    "        return convert_bytes(file_info.st_size)\n",
    "\n",
    "\n",
    "# Lets check the file size of MS Paint exe \n",
    "# or you can use any file path\n",
    "\n",
    "file_path = r\"C:\\\\Users\\\\virus\\\\Documents\\\\Win11_22H2_French_x64v1.iso\"\n",
    "\n",
    "#file_path = r\"C:\\\\Users\\\\virus\\\\Downloads\\\\PowerBIReportServer (1).exe\"\n",
    "#file_path = r\"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\\\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html\"\n",
    "print(file_size(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = \"INFO\"\n",
    "linkedin = \"LINKEDIN\"\n",
    "path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\"\n",
    "endswith = \".html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html\n",
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13799-INFO-EMP-LINKEDIN-FR-1555991658.html\n"
     ]
    }
   ],
   "source": [
    "all_path_linkedin_file = GetPathLinkedinFiles(path,endswith,info,linkedin)\n",
    "print(all_path_linkedin_file[0])\n",
    "print(all_path_linkedin_file[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>title</th>\n",
       "      <th>society</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INF...</td>\n",
       "      <td>IngÃ©nieur Expert(e) Talend (H/F) / Freelance</td>\n",
       "      <td>Freelance-info.fr</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13547-INF...</td>\n",
       "      <td>INGÃ‰NIEUR COMMERCIAL</td>\n",
       "      <td>Groupe PSIH</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13548-INF...</td>\n",
       "      <td>IngÃ©nieur Expert(e) Talend F/H</td>\n",
       "      <td>CELAD</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13549-INF...</td>\n",
       "      <td>IngÃ©nieur Expert(e) Talend (H/F)</td>\n",
       "      <td>CELAD</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13550-INF...</td>\n",
       "      <td>Consultant Big Data</td>\n",
       "      <td>Sopra Steria</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  \\\n",
       "0  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INF...   \n",
       "1  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13547-INF...   \n",
       "2  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13548-INF...   \n",
       "3  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13549-INF...   \n",
       "4  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13550-INF...   \n",
       "\n",
       "                                          title            society      city  \n",
       "0  IngÃ©nieur Expert(e) Talend (H/F) / Freelance  Freelance-info.fr  Lyon, FR  \n",
       "1                          INGÃ‰NIEUR COMMERCIAL        Groupe PSIH  Lyon, FR  \n",
       "2                IngÃ©nieur Expert(e) Talend F/H              CELAD  Lyon, FR  \n",
       "3              IngÃ©nieur Expert(e) Talend (H/F)              CELAD  Lyon, FR  \n",
       "4                           Consultant Big Data       Sopra Steria  Lyon, FR  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = GetDataFromLinkedinFiles(all_path_linkedin_file)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"C:\\\\TD_DATALAKE\\DATALAKE\\\\1_LANDING_ZONE\\\\LINKEDIN\\EMP\\\\title.csv\",index=None,encoding=\"utf-8\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\TD_DATALAKE\\DATALAKE\\\\1_LANDING_ZONE\\\\LINKEDIN\\EMP\\\\title.csv\",index=True,encoding=\"utf-8\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\\\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = open(url,encoding=\"utf-8\",errors=\"replace\")\n",
    "soup = bs(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"job-criteria__list\">\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Niveau hiÃ©rarchique</h3><span class=\"job-criteria__text job-criteria__text--criteria\">Premier emploi</span>\n",
       "</li>\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Type dâ€™emploi</h3><span class=\"job-criteria__text job-criteria__text--criteria\">Temps plein</span>\n",
       "</li>\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Fonction</h3><span class=\"job-criteria__text job-criteria__text--criteria\">IngÃ©nierie</span><span class=\"job-criteria__text job-criteria__text--criteria\">Technologies de lâ€™information</span>\n",
       "</li>\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Secteurs</h3><span class=\"job-criteria__text job-criteria__text--criteria\">Technologies et services de\n",
       "                            lâ€™information</span><span class=\"job-criteria__text job-criteria__text--criteria\">Logiciels\n",
       "                            informatiques</span><span class=\"job-criteria__text job-criteria__text--criteria\">Recrutement </span>\n",
       "</li>\n",
       "</ul>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2 = soup.find(\"ul\",{'class':\"job-criteria__list\"})\n",
    "test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h3 class=\"job-criteria__subheader\">Niveau hiÃ©rarchique</h3>, <h3 class=\"job-criteria__subheader\">Type dâ€™emploi</h3>, <h3 class=\"job-criteria__subheader\">Fonction</h3>, <h3 class=\"job-criteria__subheader\">Secteurs</h3>]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "## http://carrefax.com/new-blog/2018/9/5/find-child-elements-using-beautifulsoup\n",
    "\n",
    "test = test_2.find_all(\"h3\",{'class':'job-criteria__subheader'})\n",
    "print(test)\n",
    "\n",
    "for i in test:\n",
    "    children = i.findChildren('span',{'class':\"job-criteria__text job-criteria__text--criteria\"},recursive=True)\n",
    "    print(children)\n",
    "    for child in children:\n",
    "        print(child)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test.find_allChildren('span',{'class':\"job-criteria__text job-criteria__text--criteria\"},recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NavigableString' object has no attribute 'findChildren'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m test:\n\u001b[1;32m----> 3\u001b[0m     children \u001b[39m=\u001b[39m i\u001b[39m.\u001b[39;49mfindChildren(\u001b[39m\"\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m\"\u001b[39m,{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mjob-criteria__text job-criteria__text--criteria\u001b[39m\u001b[39m\"\u001b[39m},recursive\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m children:\n\u001b[0;32m      5\u001b[0m         i_want \u001b[39m=\u001b[39m child\u001b[39m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\TD_DATALAKE\\venv\\Lib\\site-packages\\bs4\\element.py:965\u001b[0m, in \u001b[0;36mNavigableString.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m    964\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    966\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[0;32m    967\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NavigableString' object has no attribute 'findChildren'"
     ]
    }
   ],
   "source": [
    "for i in test:\n",
    "\n",
    "    children = i.findChildren(\"span\",{'class':\"job-criteria__text job-criteria__text--criteria\"},recursive=True)\n",
    "    for child in children:\n",
    "        i_want = child.text\n",
    "        print(i_want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Niveau hiÃ©rarchique']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child = test.contents\n",
    "child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IngÃ©nieur Expert(e) Talend (H/F) / Freelance\n"
     ]
    }
   ],
   "source": [
    "for title in soup.find_all('h1', attrs = {'class':'topcard__title'}): \n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niveau hiÃ©rarchique\n",
      "Type dâ€™emploi\n",
      "Fonction\n",
      "Secteurs\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('h3', attrs = {'class':'job-criteria__subheader'}): \n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premier emploi\n",
      "Temps plein\n",
      "IngÃ©nierie\n",
      "Technologies de lâ€™information\n",
      "Technologies et services de\n",
      "                            lâ€™information\n",
      "Logiciels\n",
      "                            informatiques\n",
      "Recrutement \n"
     ]
    }
   ],
   "source": [
    "for j in soup.find_all('span', attrs = {'class':'job-criteria__text job-criteria__text--criteria'}):\n",
    "    print(j.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issu du monde bancaire, CELAD a Ã©tÃ© crÃ©Ã© en 1990\n",
      "                    Ã  Toulouse. Nos 1350 collaborateurs(trices) interviennent aujourdâ€™hui sur 2 pÃ´les dâ€™activitÃ©s : les\n",
      "                    systÃ¨mes dâ€™information et lâ€™informatique industrielle et ceci chez plus de 250 clients (PME/PMI et\n",
      "                    grands comptes).Depuis 2006, lâ€™entitÃ© CELAD Auvergne-RhÃ´ne-Alpes propose de nombreuses missions\n",
      "                    intÃ©ressantes et Ã  forte valeur ajoutÃ©e sur lâ€™ensemble de la rÃ©gion : Clermont-Ferrand, MontluÃ§on,\n",
      "                    Lyon, Grenoble, St-Etienne, Valence, Dijon et Archamps.Nous sommes entiÃ¨rement impliquÃ©(e)s dans\n",
      "                    la gestion de nos collaborateurs(trices), ce qui est la base de notre dÃ©veloppement ainsi que le\n",
      "                    reflet de la qualitÃ© de notre travail.DotÃ©e dâ€™un vÃ©ritable processus de recrutement, lâ€™agence\n",
      "                    CELAD Auvergne-RhÃ´ne-Alpes accompagne ses candidat(e)s avec beaucoup dâ€™enthousiasme et de passion au\n",
      "                    quotidien.Donc si vous souhaitez faire partie de lâ€™aventure Celadienne, rejoignez-nous\n",
      "                    !Dâ€™ailleurs, nous poursuivons notre dÃ©veloppement et recrutons actuellement un(e) IngÃ©nieur\n",
      "                    Expert(e) Talend pour intervenir chez un de nos clients.Contexte : Vous interviendrez sur divers\n",
      "                    sujets liÃ©s Ã  la gestion des EDI du Domaine DÃ©cisionnel.Vos Principales Missions\n",
      "                            Quotidiennes\n",
      "\n",
      " La convergence sur une plate-forme technique unique de lâ€™ensemble des flux EDI existants.\n",
      "                        \n",
      " La mise en Âœuvre avec nos partenaires dâ€™EDI disponibles et lâ€™accompagnement de ce processus\n",
      "                            de bout en bout. Cet accompagnement intÃ¨gre :\n",
      "la communication et lâ€™explicitation de nos spÃ©cifications sur les formats de flux et sur les\n",
      "                    modalitÃ©s techniques de communication sÃ©curisÃ©e auprÃ¨s des partenaires concernÃ©sla recette des\n",
      "                    flux produits par nos partenaireslâ€™accompagnement de la mise en production et de lâ€™intÃ©gration\n",
      "                    sous la plate-forme de surveillance et de communication.\n",
      "\n",
      " La participation potentielle Ã  nos projets dâ€™extension de lâ€™offre de service EDI afin de\n",
      "                            couvrir de nouveaux besoins dâ€™Ã©change de donnÃ©es.\n",
      "Environnement Technique\n",
      "\n",
      " Expertise dÃ©cisionnelle\n",
      " Expertise Talend\n",
      " Connaissance des mÃ©tiers de lâ€™Assurance serait un plus apprÃ©ciÃ© mais non indispensable\n",
      "\n",
      "Formation : Bac +5 et expÃ©rience de 5 ans minimum sur un poste similaire\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "for libelle_emp in soup.find_all('div', attrs = {\"description__text description__text--rich\"}) :\n",
    "    print(libelle_emp.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\n",
      "13546-INFO-EMP-LINKEDIN-FR-1599984246.html\n",
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\"\n",
    "list_dir_path = os.listdir(\"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\")\n",
    "list_file = [x for x in list_dir_path if x.endswith(\".html\")]\n",
    "list_file_info = [x for x in list_dir_path if x.endswith(\".html\") and x.__contains__(\"INFO\")]\n",
    "\n",
    "print(str(path))\n",
    "\n",
    "print(list_file_info[0])\n",
    "\n",
    "url = (str(path)+\"\\\\\"+str(list_file_info[0]))\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freelance-info.fr recrute pour des postes de IngÃ©nieur Expert(e) Talend (H/F) / Freelance (Lyon, FR) | LinkedIn\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('title').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IngÃ©nieur Expert(e) Talend (H/F) / Freelance (France)RechercherOffres dâ€™emploiRÃ©seauIgnorerIgnorerIgnorerIgnorer\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('section').get_text())\n",
    "\n",
    "#soup.prettify()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ba4306b892c702d24615d398cf592267eb03fb7bf2d599eb2ccdad9d12df570"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
