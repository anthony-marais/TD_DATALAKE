{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import platform\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mclick\u001b[39;00m\n\u001b[1;32m      4\u001b[0m os_system \u001b[39m=\u001b[39m GetOS()\n\u001b[0;32m----> 5\u001b[0m path_project \u001b[39m=\u001b[39m GenerateOSPathProject(os_system)\n\u001b[1;32m      6\u001b[0m source_path \u001b[39m=\u001b[39m path_project[\u001b[39m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m logfiles_path \u001b[39m=\u001b[39m path_project[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Dev/GitHub/TD_DATALAKE/DVLP/CODING/python_function_datalake.py:32\u001b[0m, in \u001b[0;36mGenerateOSPathProject\u001b[0;34m(os_system)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m os_system \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWindows\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     30\u001b[0m     user \u001b[39m=\u001b[39m GetUser()\n\u001b[1;32m     31\u001b[0m     source_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49menviron\u001b[39m.\u001b[39;49mget(\n\u001b[0;32m---> 32\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39msource_path_linux\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39m\u001b[39m//\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39muser\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m     logfiles_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\n\u001b[1;32m     34\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlogfiles_path_linux\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m//\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39muser\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m     landing_zone_linkedin_emp_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\n\u001b[1;32m     36\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlanding_zone_linkedin_emp_path_linux\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m//\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39muser\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "from python_function_datalake import MoveFileToLandingZone, GetOS,GenerateOSPathProject\n",
    "import click\n",
    "\n",
    "os_system = GetOS()\n",
    "path_project = GenerateOSPathProject(os_system)\n",
    "source_path = path_project[0]\n",
    "logfiles_path = path_project[1]\n",
    "landing_zone_linkedin_emp_path = path_project[2]\n",
    "landing_zone_glassdoor_soc_path = path_project[3]\n",
    "landing_zone_glassdoor_avi_path = path_project[4]\n",
    "linkedin_contains = path_project[5]\n",
    "glassdoor_soc_contains = path_project[6]\n",
    "glassdoor_avi_contains = path_project[7]\n",
    "path = path_project[8]\n",
    "endswith = path_project[9]\n",
    "contains_1 = path_project[10]\n",
    "contains_2 = path_project[11]\n",
    "delimiter_path = path_project[12]\n",
    "curated_zone_linkedin_emp_path = path_project[13]\n",
    "metadate_file_name_path = path_project[14]\n",
    "curated_zone_glassdoor_avis_path = path_project[15]\n",
    "\n",
    "\n",
    "#print(source_path)\n",
    "#print(logfiles_path)\n",
    "#print(landing_zone_linkedin_emp_path)\n",
    "#print(landing_zone_glassdoor_soc_path)\n",
    "#print(landing_zone_glassdoor_avi_path)\n",
    "#print(linkedin_contains)\n",
    "#print(glassdoor_soc_contains)\n",
    "#print(glassdoor_avi_contains)\n",
    "#print(path)\n",
    "#print(endswith)\n",
    "#print(contains_1)\n",
    "#print(contains_2)\n",
    "#print(delimiter_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MoveFileToLandingZone(source_path, endswith, linkedin_contains, glassdoor_soc_contains, glassdoor_avi_contains,\n",
    "                      landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path, logfiles_path, delimiter_path,metadate_file_name_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metadata_files = \"/home/virus/Documents/TD_DATALAKE/LOGFILES/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadMetadataFiles(metadate_file_name_path):\n",
    "    \"\"\"\n",
    "    Load metadata files\n",
    "    \"\"\"\n",
    "    metadata = pd.read_csv(metadate_file_name_path)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_ID</th>\n",
       "      <th>File_Path_Origin</th>\n",
       "      <th>File_Size</th>\n",
       "      <th>File_Date</th>\n",
       "      <th>File_Path_Destination</th>\n",
       "      <th>File_Destination_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>320.6 KB</td>\n",
       "      <td>2023-04-03 20:33:34.478083</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>320.6 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>223.4 KB</td>\n",
       "      <td>2023-04-03 20:33:34.481893</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>223.4 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>550.1 KB</td>\n",
       "      <td>2023-04-03 20:33:34.485656</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>550.1 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>196.9 KB</td>\n",
       "      <td>2023-04-03 20:33:34.488485</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>196.9 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>326.9 KB</td>\n",
       "      <td>2023-04-03 20:33:34.491768</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>326.9 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>599</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>484.0 KB</td>\n",
       "      <td>2023-04-03 20:33:35.481891</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>484.0 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>600</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>233.2 KB</td>\n",
       "      <td>2023-04-03 20:33:35.483621</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>233.2 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>601</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>202.5 KB</td>\n",
       "      <td>2023-04-03 20:33:35.485088</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>202.5 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>602</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>195.6 KB</td>\n",
       "      <td>2023-04-03 20:33:35.486653</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>195.6 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>603</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>217.7 KB</td>\n",
       "      <td>2023-04-03 20:33:35.488176</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>217.7 KB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_ID                                   File_Path_Origin File_Size  \\\n",
       "0          1  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  320.6 KB   \n",
       "1          2  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  223.4 KB   \n",
       "2          3  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  550.1 KB   \n",
       "3          4  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  196.9 KB   \n",
       "4          5  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  326.9 KB   \n",
       "..       ...                                                ...       ...   \n",
       "598      599  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  484.0 KB   \n",
       "599      600  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  233.2 KB   \n",
       "600      601  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  202.5 KB   \n",
       "601      602  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  195.6 KB   \n",
       "602      603  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  217.7 KB   \n",
       "\n",
       "                      File_Date  \\\n",
       "0    2023-04-03 20:33:34.478083   \n",
       "1    2023-04-03 20:33:34.481893   \n",
       "2    2023-04-03 20:33:34.485656   \n",
       "3    2023-04-03 20:33:34.488485   \n",
       "4    2023-04-03 20:33:34.491768   \n",
       "..                          ...   \n",
       "598  2023-04-03 20:33:35.481891   \n",
       "599  2023-04-03 20:33:35.483621   \n",
       "600  2023-04-03 20:33:35.485088   \n",
       "601  2023-04-03 20:33:35.486653   \n",
       "602  2023-04-03 20:33:35.488176   \n",
       "\n",
       "                                 File_Path_Destination File_Destination_Size  \n",
       "0    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              320.6 KB  \n",
       "1    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              223.4 KB  \n",
       "2    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              550.1 KB  \n",
       "3    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              196.9 KB  \n",
       "4    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              326.9 KB  \n",
       "..                                                 ...                   ...  \n",
       "598  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              484.0 KB  \n",
       "599  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              233.2 KB  \n",
       "600  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              202.5 KB  \n",
       "601  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              195.6 KB  \n",
       "602  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              217.7 KB  \n",
       "\n",
       "[603 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = LoadMetadataFiles(metadate_file_name_path)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(metadate_file_name_path, curated_zone_linkedin_emp_path, curated_zone_glassdoor_avis_path):\n",
    "    ''' Function to get data from html file where the file name contains a specific string (INFO) AND (LINKEDIN) '''\n",
    "\n",
    "    \"\"\"\n",
    "    Call LoadMetadataFiles function\n",
    "    \"\"\"\n",
    "    landing_zone_path = LoadMetadataFiles(metadate_file_name_path)\n",
    "\n",
    "    linkedin_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\n",
    "        \"LINKEDIN\")]\n",
    "    len_linkedin_files = len(linkedin_files)\n",
    "    glassdoor_avis_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\n",
    "        \"AVIS-SOC\")]\n",
    "    len_glassdoor_avis_files = len(glassdoor_avis_files)\n",
    "    glassdoor_soc_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\n",
    "        \"INFO-SOC\")]\n",
    "    len_glassdoor_soc_files = len(glassdoor_soc_files)\n",
    "\n",
    "    print(\"Number of Linkedin files: \", len_linkedin_files)\n",
    "    print(\"Number of Glassdoor avis files: \", len_glassdoor_avis_files)\n",
    "    print(\"Number of Glassdoor soc files: \", len_glassdoor_soc_files)\n",
    "    list_of_data = []\n",
    "    title_file = []\n",
    "    columns_dataframe = [\"file\", \"title\", \"society\", \"city\", \"description\"]\n",
    "    data_title = []\n",
    "    data_society = []\n",
    "    data_city = []\n",
    "    data_country = []\n",
    "    data_description = []\n",
    "\n",
    "    list_data_avis = []\n",
    "    columns_dataframe_avis = [\"file\", \"society\", \"date\", \"avis_positif\",\n",
    "                              \"avis_negatif\", \"status\", \"poste\", \"localisation\", \"mean_rate\", \"general_rate\"]\n",
    "    title_avis = []\n",
    "    avis_society = []\n",
    "    date_avis_glassdoor = []\n",
    "    avis_positif_list = []\n",
    "    avis_negatif_list = []\n",
    "    status_list = []\n",
    "    poste_list = []\n",
    "    localisation_list = []\n",
    "    mean_rate_list = []\n",
    "    rate_list = []\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for i in linkedin_files[\"File_Path_Destination\"]:\n",
    "        title_file.append(i)\n",
    "\n",
    "        with open(i, 'r', encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            soup = bs(f, 'html.parser')\n",
    "            title = [i for i in soup.find_all(\n",
    "                'h1', attrs={'class': 'topcard__title'})]\n",
    "            title_text = title[0].text\n",
    "            society = [i for i in soup.find_all(\n",
    "                'span', attrs={'class': 'topcard__flavor'})]\n",
    "            society_text = society[0].text\n",
    "            city = [i for i in soup.find_all(\n",
    "                'span', attrs={'class': 'topcard__flavor topcard__flavor--bullet'})]\n",
    "            description = [i for i in soup.find_all(\n",
    "                'div', attrs={'class': 'description__text description__text--rich'})]\n",
    "\n",
    "            city_text = city[0].text\n",
    "            description_text = description[0].text\n",
    "            data_title.append(title_text)\n",
    "            data_society.append(society_text)\n",
    "            data_city.append(city_text)\n",
    "            data_description.append(description_text)\n",
    "\n",
    "    print(\"Number of Linkedin files process: \", len_linkedin_files, \"\\n\")\n",
    "    data_linkedin = pd.DataFrame(list(zip(\n",
    "        title_file, data_title, data_society, data_city, data_description)), columns=columns_dataframe)\n",
    "    data_linkedin.index = np.arange(1, len(data_linkedin) + 1)\n",
    "    data_linkedin.to_csv(curated_zone_linkedin_emp_path +\n",
    "                         \"/data_linkedin.csv\", index=True, encoding=\"utf-8\", header=True)\n",
    "    print(\"Dataframe Linkedin saved in: \",\n",
    "          curated_zone_linkedin_emp_path+\"/data_linkedin.csv\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for i in glassdoor_avis_files[\"File_Path_Destination\"]:\n",
    "        title_avis.append(i)\n",
    "        with open(i, 'r', encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            soup = bs(f, 'html.parser')\n",
    "            avis_society = [i for i in soup.find_all(\n",
    "                'p', attrs={\"class\": \"h1 strong tightAll\"})]\n",
    "            avis_society_text = avis_society[0].text\n",
    "            date_avis = [i for i in soup.find_all(\n",
    "                'time', attrs={'class': 'date subtle small'})][0].text\n",
    "            avis_positif = [i for i in soup.find(\n",
    "                string='Avantages').findNext('p')]\n",
    "            avis_negatif = [i for i in soup.find(\n",
    "                string='Inconvénients').findNext('p')]\n",
    "            result_avis_positif = avis_positif if len(\n",
    "                avis_positif) > 0 else \"None\"\n",
    "            result_avis_negatif = avis_negatif if len(\n",
    "                avis_negatif) > 0 else \"None\"\n",
    "\n",
    "            status = [i for i in soup.find_all(\n",
    "                'span', attrs={\"class\": \"authorJobTitle middle reviewer\"})][0].text\n",
    "            status_text = status.split(\"-\")[0].strip()\n",
    "            poste = status.split(\n",
    "                \"-\")[1].strip() if len(status.split(\"-\")) > 1 else \"None\"\n",
    "            localisation = [i for i in soup.find_all(\n",
    "                'span', attrs={\"class\": \"authorLocation\"})]\n",
    "            result_localisation = localisation[0].text if len(\n",
    "                localisation) > 0 else \"None\"\n",
    "            mean_rate = [i for i in soup.find_all('div', attrs={\n",
    "                                                  \"class\": \"v2__EIReviewsRatingsStylesV2__ratingNum v2__EIReviewsRatingsStylesV2__large\"})]\n",
    "\n",
    "            rate_match = soup.find_all('span', attrs={\n",
    "                                       \"class\": \"gdStars gdRatings sm mr-sm mr-md-std stars__StarsStyles__gdStars\"})[0].span.contents[0]\n",
    "\n",
    "            rate = re.sub(\n",
    "                r'<span class=\"(.*)\" title=\"(.*)\">(.*)</span>(.*)', r'\\2', str(rate_match))\n",
    "\n",
    "            list_data_avis.append(avis_society_text)\n",
    "            date_avis_glassdoor.append(date_avis)\n",
    "            avis_positif_list.append(result_avis_positif[0].text)\n",
    "            avis_negatif_list.append(result_avis_negatif[0].text)\n",
    "            status_list.append(status_text)\n",
    "            poste_list.append(poste)\n",
    "            localisation_list.append(result_localisation)\n",
    "            mean_rate_list.append(mean_rate[0].text)\n",
    "            rate_list.append(rate)\n",
    "\n",
    "    print(\"Number of Glassdoor avis files process: \", len_glassdoor_avis_files, \"\\n\")\n",
    "    data_glassdoor_avis = pd.DataFrame(list(zip(title_avis, list_data_avis, date_avis_glassdoor, avis_positif_list,\n",
    "                                       avis_negatif_list, status_list, poste_list, localisation_list, mean_rate_list, rate_list)), columns=columns_dataframe_avis)\n",
    "    data_glassdoor_avis.index = np.arange(1, len(data_glassdoor_avis) + 1)\n",
    "    data_glassdoor_avis.to_csv(\n",
    "        curated_zone_glassdoor_avis_path+\"/data_glassdoor_avis.csv\", index=True, encoding=\"utf-8\", header=True)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"Dataframe avis glassdoor saved in: \",\n",
    "          curated_zone_glassdoor_avis_path+\"/data_glassdoor_avis.csv\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in glassdoor_soc_files[\"File_Path_Destination\"]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetData(metadate_file_name_path, curated_zone_linkedin_emp_path, curated_zone_glassdoor_avis_path):\n",
    "    ''' Function to get data from html file where the file name contains a specific string (INFO) AND (LINKEDIN) '''\n",
    "\n",
    "    \"\"\"\n",
    "    Call LoadMetadataFiles function\n",
    "    \"\"\"\n",
    "    landing_zone_path = LoadMetadataFiles(metadate_file_name_path)\n",
    "\n",
    "    linkedin_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\n",
    "        \"LINKEDIN\")]\n",
    "    len_linkedin_files = len(linkedin_files)\n",
    "    glassdoor_avis_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\n",
    "        \"AVIS-SOC\")]\n",
    "    len_glassdoor_avis_files = len(glassdoor_avis_files)\n",
    "    glassdoor_soc_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\n",
    "        \"INFO-SOC\")]\n",
    "    len_glassdoor_soc_files = len(glassdoor_soc_files)\n",
    "\n",
    "    print(\"Number of Linkedin files: \", len_linkedin_files)\n",
    "    print(\"Number of Glassdoor avis files: \", len_glassdoor_avis_files)\n",
    "    print(\"Number of Glassdoor soc files: \", len_glassdoor_soc_files)\n",
    "    list_of_data = []\n",
    "    title_file = []\n",
    "    columns_dataframe = [\"file\", \"title\", \"society\", \"city\", \"description\"]\n",
    "    data_title = []\n",
    "    data_society = []\n",
    "    data_city = []\n",
    "    data_country = []\n",
    "    data_description = []\n",
    "\n",
    "    list_data_avis = []\n",
    "    columns_dataframe_avis = [\"file\", \"society\", \"date\", \"avis_positif\",\n",
    "                              \"avis_negatif\", \"status\", \"poste\", \"localisation\", \"mean_rate\", \"general_rate\"]\n",
    "    title_avis = []\n",
    "    avis_society = []\n",
    "    date_avis_glassdoor = []\n",
    "    avis_positif_list = []\n",
    "    avis_negatif_list = []\n",
    "    status_list = []\n",
    "    poste_list = []\n",
    "    localisation_list = []\n",
    "    mean_rate_list = []\n",
    "    rate_list = []\n",
    "\n",
    "\n",
    "    glassdoor_society_name_list = []\n",
    "    title_glassdoor_soc = []\n",
    "    glassdoor_soc_city_list = []\n",
    "    glassdoor_soc_size_list = []\n",
    "    glassdoor_secteur_list = []\n",
    "\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in glassdoor_soc_files[\"File_Path_Destination\"]:\n",
    "        title_glassdoor_soc.append(i)\n",
    "        with open(i, 'r', encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            soup = bs(f, 'html.parser')\n",
    "\n",
    "            glassdoor_society_name = [i for i in soup.find_all('h1', attrs={'class': \"strong tightAll\"})[0]]\n",
    "            glassdoor_society_name_list.append(glassdoor_society_name[0].text)\n",
    "            glassdoor_soc_city_list.append(soup.find_all('div', attrs={'class': \"infoEntity\"})[1].span.text)\n",
    "            glassdoor_soc_size_list.append(soup.find_all('div', attrs={'class': \"infoEntity\"})[2].span.text)\n",
    "            glassdoor_secteur_list.append(soup.find(ext='Secteur').findNext('span').text)\n",
    "\n",
    "    data_glassdoor_soc = pd.DataFrame(list(zip(title_glassdoor_soc, glassdoor_society_name_list,glassdoor_soc_city_list,glassdoor_soc_size_list,glassdoor_secteur_list)), columns=[\"file\", \"society\", \"city\",\"size\",\"secteur\"])\n",
    "    \n",
    "\n",
    "    return data_glassdoor_soc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Linkedin files:  254\n",
      "Number of Glassdoor avis files:  209\n",
      "Number of Glassdoor soc files:  140\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findNext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m GetData(metadate_file_name_path, curated_zone_linkedin_emp_path, curated_zone_glassdoor_avis_path)\n",
      "Cell \u001b[0;32mIn[12], line 69\u001b[0m, in \u001b[0;36mGetData\u001b[0;34m(metadate_file_name_path, curated_zone_linkedin_emp_path, curated_zone_glassdoor_avis_path)\u001b[0m\n\u001b[1;32m     67\u001b[0m         glassdoor_soc_city_list\u001b[39m.\u001b[39mappend(soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, attrs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39minfoEntity\u001b[39m\u001b[39m\"\u001b[39m})[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mspan\u001b[39m.\u001b[39mtext)\n\u001b[1;32m     68\u001b[0m         glassdoor_soc_size_list\u001b[39m.\u001b[39mappend(soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, attrs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39minfoEntity\u001b[39m\u001b[39m\"\u001b[39m})[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mspan\u001b[39m.\u001b[39mtext)\n\u001b[0;32m---> 69\u001b[0m         glassdoor_secteur_list\u001b[39m.\u001b[39mappend(soup\u001b[39m.\u001b[39;49mfind(ext\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSecteur\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mfindNext(\u001b[39m'\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mtext)\n\u001b[1;32m     71\u001b[0m data_glassdoor_soc \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(title_glassdoor_soc, glassdoor_society_name_list,glassdoor_soc_city_list,glassdoor_soc_size_list,glassdoor_secteur_list)), columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msociety\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcity\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39msize\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39msecteur\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     74\u001b[0m \u001b[39mreturn\u001b[39;00m data_glassdoor_soc\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findNext'"
     ]
    }
   ],
   "source": [
    "GetData(metadate_file_name_path, curated_zone_linkedin_emp_path, curated_zone_glassdoor_avis_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glassdoor_city_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m glassdoor_city_list\u001b[39m.\u001b[39mappend(soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, attrs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39minfoEntity\u001b[39m\u001b[39m\"\u001b[39m})[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mspan\u001b[39m.\u001b[39mtext)\n\u001b[1;32m      3\u001b[0m glassdoor_society_name \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mh1\u001b[39m\u001b[39m'\u001b[39m, attrs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mstrong tightAll\u001b[39m\u001b[39m\"\u001b[39m})[\u001b[39m0\u001b[39m]]\n\u001b[1;32m      4\u001b[0m glassdoor_society_name_list\u001b[39m.\u001b[39mappend(glassdoor_society_name[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glassdoor_city_list' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "            glassdoor_city_list.append(soup.find_all('div', attrs={'class': \"infoEntity\"})[1].span.text)\n",
    "\n",
    "            glassdoor_society_name = [i for i in soup.find_all('h1', attrs={'class': \"strong tightAll\"})[0]]\n",
    "            glassdoor_society_name_list.append(glassdoor_society_name[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Services informatiques\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8105/852838445.py:7: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  myTest = soup.find(text='Secteur').findNext('span').text\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/SOC/13778-INFO-SOC-GLASSDOOR-E9028_P1.html\", 'r', encoding=\"utf-8\",errors=\"replace\") as f:\n",
    "#    soup = bs(f, 'html.parser')\n",
    "    soup = bs(f, 'html.parser')\n",
    "\n",
    "    list = []\n",
    "\n",
    "    myTest = soup.find(text='Secteur').findNext('span').text\n",
    "    print(myTest)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = \"\"\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "list = []\n",
    "\n",
    "\n",
    "for combination in itertools.product(range(10), repeat=6):\n",
    "    \n",
    "    list.append(''.join(map(str, combination)))\n",
    "\n",
    "\n",
    "pd.DataFrame(list).to_csv(\"/home/virus/Documents/phone_number.csv\", index=True, encoding=\"utf-8\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'163450'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list[163450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linux'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetOS():\n",
    "    system = platform.system()\n",
    "    return system\n",
    "\n",
    "os_system = GetOS()\n",
    "os_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linux'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = platform.system()\n",
    "system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUser():\n",
    "    user = getpass.getuser()\n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateOSPathProject(os_system):\n",
    "\n",
    "    load_dotenv()\n",
    "    \n",
    "    if os_system != \"Windows\":\n",
    "        user = GetUser()\n",
    "        source_path = os.environ.get('source_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        logfiles_path = os.environ.get('logfiles_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        landing_zone_linkedin_emp_path = os.environ.get(\n",
    "            'landing_zone_linkedin_emp_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        landing_zone_glassdoor_soc_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_soc_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        landing_zone_glassdoor_avi_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_avi_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        linkedin_contains = os.environ.get('linkedin_contains')\n",
    "        glassdoor_soc_contains = os.environ.get('glassdoor_soc_contains')\n",
    "        glassdoor_avi_contains = os.environ.get('glassdoor_avi_contains')\n",
    "        path = os.environ.get('path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        endswith = os.environ.get('endswith')\n",
    "        contains_1 = os.environ.get('contains_1')\n",
    "        contains_2 = os.environ.get('contains_2')\n",
    "        delimiter_path = \"/\"\n",
    "\n",
    "    elif os_system == \"Windows\":\n",
    "        source_path = os.environ.get('source_path_windows')\n",
    "        logfiles_path = os.environ.get('logfiles_path_windows')\n",
    "        landing_zone_linkedin_emp_path = os.environ.get(\n",
    "            'landing_zone_linkedin_emp_path_windows')\n",
    "        landing_zone_glassdoor_soc_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_soc_path_windows')\n",
    "        landing_zone_glassdoor_avi_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_avi_path_windows')\n",
    "        linkedin_contains = os.environ.get('linkedin_contains')\n",
    "        glassdoor_soc_contains = os.environ.get('glassdoor_soc_contains')\n",
    "        glassdoor_avi_contains = os.environ.get('glassdoor_avi_contains')\n",
    "        path = os.environ.get('path_windows')\n",
    "        endswith = os.environ.get('endswith')\n",
    "        contains_1 = os.environ.get('contains_1')\n",
    "        contains_2 = os.environ.get('contains_2')\n",
    "        delimiter_path = \"\\\\\"\n",
    "\n",
    "\n",
    "    return source_path, logfiles_path, landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path, linkedin_contains, glassdoor_soc_contains, glassdoor_avi_contains, path, endswith, contains_1, contains_2, delimiter_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'source_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os_path_project \u001b[39m=\u001b[39m GenerateOSPathProject(os_system)\u001b[39m.\u001b[39;49msource_path\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'source_path'"
     ]
    }
   ],
   "source": [
    "os_path_project = GenerateOSPathProject(os_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'source_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m source_path\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_path' is not defined"
     ]
    }
   ],
   "source": [
    "source_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPath(os_path_project):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateWotkingPath(source_path,user):\n",
    "    working_path = source_path.replace(\"//\", \"/\"+user+\"/\")\n",
    "    return working_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenerateWotkingPath(source_path,user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUser():\n",
    "    user = getpass.getuser()\n",
    "    return user\n",
    "\n",
    "def GetOS():\n",
    "    system = platform.system()\n",
    "    if system == \"/\":\n",
    "        return \"Linux\"\n",
    "    elif system == \"Windows\":\n",
    "        return \"\\\\\"\n",
    "    else:\n",
    "        return \"/\"\n",
    "\n",
    "def DefinePathOS(path):\n",
    "    delimiter_path = GetOS()\n",
    "    if delimiter_path == \"Linux\":\n",
    "        return path.replace(\"\\\\\", \"/\")\n",
    "    elif delimiter_path == \"Windows\":\n",
    "        return path.replace(\"/\", \"\\\\\")\n",
    "    else:\n",
    "        return path.replace(\"\\\\\", \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetOS():\n",
    "    system = platform.system()\n",
    "    if system == \"/\":\n",
    "        return \"Linux\"\n",
    "    elif system == \"Windows\":\n",
    "        return \"\\\\\"\n",
    "    else:\n",
    "        return \"/\"\n",
    "\n",
    "''' Function to count number of file in a directory '''\n",
    "def GetFilesPath(path,endswith,delimiter_path):\n",
    "    count = 0\n",
    "    list_of_files = []\n",
    "    ### open Files \n",
    "    for file in os.listdir(path):\n",
    "        count += 1\n",
    "    \n",
    "    list_dir_path = [x for x in os.listdir(path) if x.endswith(endswith)]    \n",
    "    \n",
    "    for Files in range(count):\n",
    "        list_of_files.append(path+delimiter_path+list_dir_path[Files])\n",
    "\n",
    "\n",
    "    return list_of_files\n",
    "\n",
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "\n",
    "\n",
    "def file_size(file_path):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \"\"\"\n",
    "    list_file_size = []\n",
    "    for i in range(len(file_path)):\n",
    "        if os.path.isfile(file_path[i]):\n",
    "            file_info = os.stat(file_path[i])\n",
    "\n",
    "            list_file_size.append(convert_bytes(file_info.st_size))\n",
    "\n",
    "    return list_file_size\n",
    "\n",
    "\n",
    "\n",
    "def MakeMetadataFile(file_path,file_size):\n",
    "    columns_list = [\"File_ID\",\"File_Path_Origin\",\"File_Size\",\"File_Date\"]\n",
    "    df = pd.DataFrame(columns = columns_list)\n",
    "    for i in range(len(file_path)):\n",
    "\n",
    "        df.loc[i] = [i+1,file_path[i],file_size[i],datetime.now()]\n",
    "\n",
    "    df.index = df.index + 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def MoveFileToLandingZone(source_path, endswith,linkedin_contains,glassdoor_soc_contains, glassdoor_avi_contains, landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path,logfiles_path,delimiter_path):\n",
    "\n",
    "    file_path = GetFilesPath(source_path,endswith,delimiter_path)\n",
    "    files_sizes = file_size(file_path)\n",
    "    general_metadata = MakeMetadataFile(file_path, files_sizes)\n",
    "    file_destination = []\n",
    "    file_destination_size = []\n",
    "\n",
    "    for file in general_metadata.File_Path_Origin:\n",
    "        if linkedin_contains in file:\n",
    "\n",
    "            file_destination.append(landing_zone_linkedin_emp_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            \n",
    "            shutil.copy(file, landing_zone_linkedin_emp_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            \n",
    "            file_destination_size.append(file_size([landing_zone_linkedin_emp_path +\n",
    "                        file.split(delimiter_path)[-1]]))\n",
    "\n",
    "        elif glassdoor_soc_contains in file:\n",
    "\n",
    "            file_destination.append(landing_zone_glassdoor_soc_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "\n",
    "\n",
    "            shutil.copy(file, landing_zone_glassdoor_soc_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            \n",
    "            file_destination_size.append(file_size([landing_zone_glassdoor_soc_path +\n",
    "                        file.split(delimiter_path)[-1]]))\n",
    "            \n",
    "        elif glassdoor_avi_contains in file:\n",
    "\n",
    "            file_destination.append(landing_zone_glassdoor_avi_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "\n",
    "            shutil.copy(file, landing_zone_glassdoor_avi_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            file_destination_size.append(file_size([landing_zone_glassdoor_avi_path +\n",
    "                        file.split(delimiter_path)[-1]]))\n",
    "            \n",
    "\n",
    "    general_metadata[\"File_Path_Destination\"] = file_destination\n",
    "    general_metadata[\"File_Destination_Size\"] = file_destination_size\n",
    "\n",
    "    general_metadata[\"File_Destination_Size\"] = general_metadata[\"File_Destination_Size\"].apply(lambda x: x[0])\n",
    "\n",
    "    general_metadata.to_csv(logfiles_path+\"/metadata-technical.csv\",index=None,encoding=\"utf-8\",header=True)\n",
    "\n",
    "\n",
    "    return general_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB\"\n",
    "logfiles_path = \"/home/virus/Documents/TD_DATALAKE/LOGFILES\"\n",
    "landing_zone_linkedin_emp_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/LINKEDIN/EMP/\"\n",
    "landing_zone_glassdoor_soc_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/SOC/\"\n",
    "landing_zone_glassdoor_avi_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/AVI/\"\n",
    "linkedin_contains = \"LINKEDIN\"\n",
    "glassdoor_soc_contains = \"INFO-SOC\"\n",
    "glassdoor_avi_contains = \"AVIS-SOC\"\n",
    "path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB\"\n",
    "endswith = \".html\"\n",
    "contains_1 = \"INFO\"\n",
    "contains_2 = \"LINKEDIN\"\n",
    "delimiter_path = GetOS()\n",
    "name_file = \"metadata-technical.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = MoveFileToLandingZone(source_path, endswith, linkedin_contains, glassdoor_soc_contains, glassdoor_avi_contains,\n",
    "                                landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path, logfiles_path, delimiter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_ID</th>\n",
       "      <th>File_Path_Origin</th>\n",
       "      <th>File_Size</th>\n",
       "      <th>File_Date</th>\n",
       "      <th>File_Path_Destination</th>\n",
       "      <th>File_Destination_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>320.6 KB</td>\n",
       "      <td>2023-02-27 19:28:44.628131</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>320.6 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>223.4 KB</td>\n",
       "      <td>2023-02-27 19:28:44.630157</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>223.4 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>550.1 KB</td>\n",
       "      <td>2023-02-27 19:28:44.634159</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>550.1 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>196.9 KB</td>\n",
       "      <td>2023-02-27 19:28:44.637487</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>196.9 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>326.9 KB</td>\n",
       "      <td>2023-02-27 19:28:44.640354</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>326.9 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>599</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>484.0 KB</td>\n",
       "      <td>2023-02-27 19:28:45.360059</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>484.0 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>600</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>233.2 KB</td>\n",
       "      <td>2023-02-27 19:28:45.361070</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>233.2 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>601</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>202.5 KB</td>\n",
       "      <td>2023-02-27 19:28:45.362049</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>202.5 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>602</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>195.6 KB</td>\n",
       "      <td>2023-02-27 19:28:45.363078</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>195.6 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>603</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>217.7 KB</td>\n",
       "      <td>2023-02-27 19:28:45.364077</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>217.7 KB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_ID                                   File_Path_Origin File_Size  \\\n",
       "1          1  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  320.6 KB   \n",
       "2          2  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  223.4 KB   \n",
       "3          3  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  550.1 KB   \n",
       "4          4  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  196.9 KB   \n",
       "5          5  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  326.9 KB   \n",
       "..       ...                                                ...       ...   \n",
       "599      599  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  484.0 KB   \n",
       "600      600  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  233.2 KB   \n",
       "601      601  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  202.5 KB   \n",
       "602      602  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  195.6 KB   \n",
       "603      603  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  217.7 KB   \n",
       "\n",
       "                     File_Date  \\\n",
       "1   2023-02-27 19:28:44.628131   \n",
       "2   2023-02-27 19:28:44.630157   \n",
       "3   2023-02-27 19:28:44.634159   \n",
       "4   2023-02-27 19:28:44.637487   \n",
       "5   2023-02-27 19:28:44.640354   \n",
       "..                         ...   \n",
       "599 2023-02-27 19:28:45.360059   \n",
       "600 2023-02-27 19:28:45.361070   \n",
       "601 2023-02-27 19:28:45.362049   \n",
       "602 2023-02-27 19:28:45.363078   \n",
       "603 2023-02-27 19:28:45.364077   \n",
       "\n",
       "                                 File_Path_Destination File_Destination_Size  \n",
       "1    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              320.6 KB  \n",
       "2    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              223.4 KB  \n",
       "3    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              550.1 KB  \n",
       "4    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              196.9 KB  \n",
       "5    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              326.9 KB  \n",
       "..                                                 ...                   ...  \n",
       "599  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              484.0 KB  \n",
       "600  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              233.2 KB  \n",
       "601  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              202.5 KB  \n",
       "602  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              195.6 KB  \n",
       "603  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              217.7 KB  \n",
       "\n",
       "[603 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadMetadataFiles(path,delimiter_path,name_file):\n",
    "    data = pd.read_csv(path+delimiter_path+name_file,encoding=\"utf-8\")\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB\"\n",
    "landing_zone_linkedin_emp_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/LINKEDIN/EMP/\"\n",
    "file_path = GetFilesPath(source_path)\n",
    "files_sizes = file_size(file_path)\n",
    "\n",
    "linkedin_contains = \"LINKEDIN\"\n",
    "\n",
    "\n",
    "\n",
    "df = MakeMetadataFile(file_path,files_sizes)\n",
    "\n",
    "\n",
    "\n",
    "''' Function moove linkedin file to landing zone '''\n",
    "def MoveFileToLandingZone(source_path,linkedin_contains,landing_zone_linkedin_emp_path):\n",
    "    \n",
    "    #list_data = []\n",
    "    columns_dataframe = [\"title\",\"society\",\"city\"]\n",
    "    data_title = []\n",
    "    data_society = []\n",
    "    data_city = []\n",
    "    \n",
    "    file_path = GetFilesPath(source_path)\n",
    "    files_sizes = file_size(file_path)\n",
    "    df = MakeMetadataFile(file_path,files_sizes)\n",
    "\n",
    "\n",
    "    for file in df.File_Name:\n",
    "        if linkedin_contains in file:\n",
    "            \n",
    "            with open(file, 'r', encoding=\"utf-8\",errors=\"replace\") as f:\n",
    "                soup = bs(f, 'html.parser')\n",
    "                title = [i for i in soup.find_all('h1', attrs = {'class':'topcard__title'})]\n",
    "                society = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor'})]\n",
    "                city = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor topcard__flavor--bullet'})]\n",
    "                data_title.append(title[0].text)\n",
    "                data_society.append(society[0].text)\n",
    "                data_city.append(city[0].text)\n",
    "                \n",
    "                df_data = pd.DataFrame(columns = columns_dataframe)\n",
    "\n",
    "                df_data = pd.DataFrame(list(zip(data_title,data_society,data_city)),columns=columns_dataframe)\n",
    "\n",
    "            shutil.copy(file,landing_zone_linkedin_emp_path+file.split(\"/\")[-1])\n",
    "\n",
    "        \n",
    "    result = pd.concat([df,df_data],axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "testing = MoveFileToLandingZone(source_path,linkedin_contains,landing_zone_linkedin_emp_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_ID</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>File_Size</th>\n",
       "      <th>File_Date</th>\n",
       "      <th>title</th>\n",
       "      <th>society</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>320.6 KB</td>\n",
       "      <td>2023-02-23 19:57:43.167425</td>\n",
       "      <td>Développeur Talend Big Data H/F</td>\n",
       "      <td>Accor</td>\n",
       "      <td>Évry, Île-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>223.4 KB</td>\n",
       "      <td>2023-02-23 19:57:43.168142</td>\n",
       "      <td>Consultant / Consultante décisionnel - Busines...</td>\n",
       "      <td>PERFECTSIGHT</td>\n",
       "      <td>Puteaux, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>550.1 KB</td>\n",
       "      <td>2023-02-23 19:57:43.169193</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Teads.tv</td>\n",
       "      <td>Paris, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>196.9 KB</td>\n",
       "      <td>2023-02-23 19:57:43.170375</td>\n",
       "      <td>CDI Ingénieur de Développement / moteurs d'opt...</td>\n",
       "      <td>GROUPE M6</td>\n",
       "      <td>Neuilly-sur-Seine, Île-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>326.9 KB</td>\n",
       "      <td>2023-02-23 19:57:43.171607</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Awalee Consulting</td>\n",
       "      <td>Région de Paris, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>600.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>233.2 KB</td>\n",
       "      <td>2023-02-23 19:57:43.795660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>601.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>202.5 KB</td>\n",
       "      <td>2023-02-23 19:57:43.796711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>602.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>195.6 KB</td>\n",
       "      <td>2023-02-23 19:57:43.797723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>603.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>217.7 KB</td>\n",
       "      <td>2023-02-23 19:57:43.798879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>INGENIEUR EN DEVELOPPEMENT BIG DATA H/F</td>\n",
       "      <td>Altran</td>\n",
       "      <td>Sophia Antipolis, FR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>604 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_ID                                          File_Name File_Size  \\\n",
       "1        1.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  320.6 KB   \n",
       "2        2.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  223.4 KB   \n",
       "3        3.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  550.1 KB   \n",
       "4        4.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  196.9 KB   \n",
       "5        5.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  326.9 KB   \n",
       "..       ...                                                ...       ...   \n",
       "600    600.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  233.2 KB   \n",
       "601    601.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  202.5 KB   \n",
       "602    602.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  195.6 KB   \n",
       "603    603.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  217.7 KB   \n",
       "0        NaN                                                NaN       NaN   \n",
       "\n",
       "                     File_Date  \\\n",
       "1   2023-02-23 19:57:43.167425   \n",
       "2   2023-02-23 19:57:43.168142   \n",
       "3   2023-02-23 19:57:43.169193   \n",
       "4   2023-02-23 19:57:43.170375   \n",
       "5   2023-02-23 19:57:43.171607   \n",
       "..                         ...   \n",
       "600 2023-02-23 19:57:43.795660   \n",
       "601 2023-02-23 19:57:43.796711   \n",
       "602 2023-02-23 19:57:43.797723   \n",
       "603 2023-02-23 19:57:43.798879   \n",
       "0                          NaT   \n",
       "\n",
       "                                                 title            society  \\\n",
       "1                      Développeur Talend Big Data H/F              Accor   \n",
       "2    Consultant / Consultante décisionnel - Busines...       PERFECTSIGHT   \n",
       "3                                       Data Scientist           Teads.tv   \n",
       "4    CDI Ingénieur de Développement / moteurs d'opt...          GROUPE M6   \n",
       "5                                       Data Scientist  Awalee Consulting   \n",
       "..                                                 ...                ...   \n",
       "600                                                NaN                NaN   \n",
       "601                                                NaN                NaN   \n",
       "602                                                NaN                NaN   \n",
       "603                                                NaN                NaN   \n",
       "0              INGENIEUR EN DEVELOPPEMENT BIG DATA H/F             Altran   \n",
       "\n",
       "                                         city  \n",
       "1                 Évry, Île-de-France, France  \n",
       "2                                 Puteaux, FR  \n",
       "3                                   Paris, FR  \n",
       "4    Neuilly-sur-Seine, Île-de-France, France  \n",
       "5                     Région de Paris, France  \n",
       "..                                        ...  \n",
       "600                                       NaN  \n",
       "601                                       NaN  \n",
       "602                                       NaN  \n",
       "603                                       NaN  \n",
       "0                        Sophia Antipolis, FR  \n",
       "\n",
       "[604 rows x 7 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.pivot_table(index=[\"title\",\"society\",\"city\"],values=[\"File_Name\"],aggfunc=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['320.6 KB']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_size([\"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/SOC/13713-AVIS-SOC-GLASSDOOR-E3142186_P1.html\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1181346619.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[68], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    for i enumerate(list):\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "for i enumerate(list):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function to get data from html file where the file name contains a specific string (INFO) AND (LINKEDIN) '''\n",
    "def GetDataFromLinkedinFiles(opened_files):\n",
    "    list_of_data = []\n",
    "    columns_dataframe = [\"file\",\"title\",\"society\",\"city\"]\n",
    "    data_title = []\n",
    "    data_society = []\n",
    "    data_city = []\n",
    "    for i in range(len(opened_files)):\n",
    "        with open(opened_files[i], 'r', encoding=\"utf-8\",errors=\"replace\") as f:\n",
    "\n",
    "            soup = bs(f, 'html.parser')\n",
    "            title = [i for i in soup.find_all('h1', attrs = {'class':'topcard__title'})]\n",
    "            society = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor'})]\n",
    "            city = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor topcard__flavor--bullet'})]\n",
    "            data_title.append(title[0].text)\n",
    "            data_society.append(society[0].text)\n",
    "            data_city.append(city[0].text)\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(opened_files,data_title,data_society,data_city)),columns=columns_dataframe)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elif \"GLASSDOOR\" and \"AVI\" in i:\n",
    "        #    shutil.copy(i,\"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/AVI/\"+i.split(\"/\")[-1])\n",
    "        #elif \"SOC\" and \"GLASSDOOR\" in i:\n",
    "        #    shutil.copy(i,\"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/SOC/\"+i.split(\"/\")[-1])\n",
    "        #else:\n",
    "        #    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/virus/Documents/TD_DATALAKE/LOGFILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glassdoor_emp_path,glassdoor_soc_path ,linkedin_contains,glassdoor_contains,glassdoor_soc_contains_1,glassdoor_soc_contains_2):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "    for i in range(len(file_path)):\n",
    "        if linkedin_contains in file_path[i] and endswith in file_path[i]:\n",
    "            shutil.copy(file_path[i],landing_zone_linkedin_emp_path+\"/\"+i)\n",
    "\n",
    "\n",
    "\n",
    "        elif contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],landing_zone_glassdoor_emp_path+\"/\"+file_path[i].split(\"\\\\\")[-1])\n",
    "        \n",
    "        elif if contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],glassdoor_soc_path+\"\\\\\"+file_path[i].split(\"\\\\\")[-1])\n",
    "\n",
    "\n",
    "def MoveGlassdoorAvisToLandingZone(file_path,glassdoor_emp_path,contains_1,contains_2):\n",
    "    for i in range(len(file_path)):\n",
    "        if contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],glassdoor_emp_path+\"\\\\\"+file_path[i].split(\"\\\\\")[-1])\n",
    "\n",
    "def MoveGlassdoorSocToLandingZone(file_path,glassdoor_soc_path,contains_1,contains_2):\n",
    "    for i in range(len(file_path)):\n",
    "        if contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],glassdoor_soc_path+\"\\\\\"+file_path[i].split(\"\\\\\")[-1])\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#file_path = GetFilesPath(path)\n",
    "#print(file_path[0])\n",
    "#landing_zone_linkedin_emp_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/LINKEDIN/EMP\"\n",
    "#landing_zone_glassdoor_emp_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\AVI\"\n",
    "#landing_zone_glassdoor_soc_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\SOC\"\n",
    "\n",
    "#endswith = \".html\"\n",
    "#linkedin_contains = \"LINKEDIN\"\n",
    "#glassdoor_contains = \"GLASSDOR\"\n",
    "#glassdoor_soc_contains_1 = \"SOC\"\n",
    "#glassdoor_soc_contains_2 = \"AVIS\"\n",
    "\n",
    "#MoveFileToLandingZone(file_path,linkedin_emp_path,glassdoor_emp_path,glassdoor_soc_path ,linkedin_contains,glassdoor_contains,glassdoor_soc_contains_1,glassdoor_soc_contains_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MoveFileToLandingZone(file_path,landing_zone_linkedin_emp_path,endswith,linkedin_contains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13546-AVIS-SOC-GLASSDOOR-E12966_P1.html'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = GetFilesPath(path)\n",
    "\n",
    "file_path[0].split(\"\\\\\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m file_path \u001b[39m=\u001b[39m GetFilesPath(path)\n\u001b[0;32m      2\u001b[0m linkedin_emp_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mTD_DATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m1_LANDING_ZONE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mLINKEDIN\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mEMP\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m glassdoor_emp_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mTD_DATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m1_LANDING_ZONE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mGLASSDOOR\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mAVI\u001b[39m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[33], line 12\u001b[0m, in \u001b[0;36mGetFilesPath\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      9\u001b[0m list_dir_path \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(path) \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mendswith(endswith)]    \n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m Files \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(count):\n\u001b[1;32m---> 12\u001b[0m     list_of_files\u001b[39m.\u001b[39mappend(path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mlist_dir_path[Files])\n\u001b[0;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m list_of_files\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "file_path = GetFilesPath(path)\n",
    "linkedin_emp_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\LINKEDIN\\\\EMP\"\n",
    "glassdoor_emp_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\AVI\"\n",
    "glassdoor_soc_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\SOC\"\n",
    "\n",
    "endswith = \".html\"\n",
    "\n",
    "\n",
    "linkedin_contains = \"LINKEDIN\"\n",
    "glassdoor_contains = \"GLASSDOR\"\n",
    "glassdoor_soc_contains_1 = \"SOC\"\n",
    "glassdoor_soc_contains_2 = \"AVIS\"\n",
    "\n",
    "MoveLinkedinFileToLandingZone(file_path,linkedin_emp_path,linkedin_contains_1,linkedin_contains_2)\n",
    "\n",
    "MoveGlassdoorSocToLandingZone(file_path,glassdoor_emp_path,glassdoor_soc_contains_1,glassdoor_soc_contains_2)\n",
    "\n",
    "MoveGlassdoorAvisToLandingZone(file_path,glassdoor_soc_path,glassdoor_soc_contains_1,glassdoor_soc_contains_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function to count number of file in a directory '''\n",
    "def GetPathLinkedinFiles(path,endswith,info,linkedin):\n",
    "    count = 0\n",
    "    list_of_files = []\n",
    "    ### open Files \n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(endswith) and file.__contains__(info) and file.__contains__(linkedin):\n",
    "            count += 1\n",
    "    list_dir_path = [x for x in os.listdir(path) if x.endswith(endswith) and x.__contains__(info) and x.__contains__(linkedin)]    \n",
    "    for FileLinkedin in range(count):\n",
    "        list_of_files.append(path+\"\\\\\"+list_dir_path[FileLinkedin])\n",
    "\n",
    "    return list_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2 GB\n"
     ]
    }
   ],
   "source": [
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "\n",
    "\n",
    "def file_size(file_path):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \"\"\"\n",
    "    if os.path.isfile(file_path):\n",
    "        file_info = os.stat(file_path)\n",
    "        return convert_bytes(file_info.st_size)\n",
    "\n",
    "\n",
    "# Lets check the file size of MS Paint exe \n",
    "# or you can use any file path\n",
    "\n",
    "file_path = r\"C:\\\\Users\\\\virus\\\\Documents\\\\Win11_22H2_French_x64v1.iso\"\n",
    "\n",
    "#file_path = r\"C:\\\\Users\\\\virus\\\\Downloads\\\\PowerBIReportServer (1).exe\"\n",
    "#file_path = r\"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\\\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html\"\n",
    "print(file_size(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = \"INFO\"\n",
    "linkedin = \"LINKEDIN\"\n",
    "path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\"\n",
    "endswith = \".html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html\n",
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13799-INFO-EMP-LINKEDIN-FR-1555991658.html\n"
     ]
    }
   ],
   "source": [
    "all_path_linkedin_file = GetPathLinkedinFiles(path,endswith,info,linkedin)\n",
    "print(all_path_linkedin_file[0])\n",
    "print(all_path_linkedin_file[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>title</th>\n",
       "      <th>society</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INF...</td>\n",
       "      <td>Ingénieur Expert(e) Talend (H/F) / Freelance</td>\n",
       "      <td>Freelance-info.fr</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13547-INF...</td>\n",
       "      <td>INGÉNIEUR COMMERCIAL</td>\n",
       "      <td>Groupe PSIH</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13548-INF...</td>\n",
       "      <td>Ingénieur Expert(e) Talend F/H</td>\n",
       "      <td>CELAD</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13549-INF...</td>\n",
       "      <td>Ingénieur Expert(e) Talend (H/F)</td>\n",
       "      <td>CELAD</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13550-INF...</td>\n",
       "      <td>Consultant Big Data</td>\n",
       "      <td>Sopra Steria</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  \\\n",
       "0  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INF...   \n",
       "1  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13547-INF...   \n",
       "2  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13548-INF...   \n",
       "3  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13549-INF...   \n",
       "4  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13550-INF...   \n",
       "\n",
       "                                          title            society      city  \n",
       "0  Ingénieur Expert(e) Talend (H/F) / Freelance  Freelance-info.fr  Lyon, FR  \n",
       "1                          INGÉNIEUR COMMERCIAL        Groupe PSIH  Lyon, FR  \n",
       "2                Ingénieur Expert(e) Talend F/H              CELAD  Lyon, FR  \n",
       "3              Ingénieur Expert(e) Talend (H/F)              CELAD  Lyon, FR  \n",
       "4                           Consultant Big Data       Sopra Steria  Lyon, FR  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = GetDataFromLinkedinFiles(all_path_linkedin_file)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"C:\\\\TD_DATALAKE\\DATALAKE\\\\1_LANDING_ZONE\\\\LINKEDIN\\EMP\\\\title.csv\",index=None,encoding=\"utf-8\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\TD_DATALAKE\\DATALAKE\\\\1_LANDING_ZONE\\\\LINKEDIN\\EMP\\\\title.csv\",index=True,encoding=\"utf-8\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\\\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = open(url,encoding=\"utf-8\",errors=\"replace\")\n",
    "soup = bs(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"job-criteria__list\">\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Niveau hiérarchique</h3><span class=\"job-criteria__text job-criteria__text--criteria\">Premier emploi</span>\n",
       "</li>\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Type d’emploi</h3><span class=\"job-criteria__text job-criteria__text--criteria\">Temps plein</span>\n",
       "</li>\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Fonction</h3><span class=\"job-criteria__text job-criteria__text--criteria\">Ingénierie</span><span class=\"job-criteria__text job-criteria__text--criteria\">Technologies de l’information</span>\n",
       "</li>\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Secteurs</h3><span class=\"job-criteria__text job-criteria__text--criteria\">Technologies et services de\n",
       "                            l’information</span><span class=\"job-criteria__text job-criteria__text--criteria\">Logiciels\n",
       "                            informatiques</span><span class=\"job-criteria__text job-criteria__text--criteria\">Recrutement </span>\n",
       "</li>\n",
       "</ul>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2 = soup.find(\"ul\",{'class':\"job-criteria__list\"})\n",
    "test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h3 class=\"job-criteria__subheader\">Niveau hiérarchique</h3>, <h3 class=\"job-criteria__subheader\">Type d’emploi</h3>, <h3 class=\"job-criteria__subheader\">Fonction</h3>, <h3 class=\"job-criteria__subheader\">Secteurs</h3>]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "## http://carrefax.com/new-blog/2018/9/5/find-child-elements-using-beautifulsoup\n",
    "\n",
    "test = test_2.find_all(\"h3\",{'class':'job-criteria__subheader'})\n",
    "print(test)\n",
    "\n",
    "for i in test:\n",
    "    children = i.findChildren('span',{'class':\"job-criteria__text job-criteria__text--criteria\"},recursive=True)\n",
    "    print(children)\n",
    "    for child in children:\n",
    "        print(child)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test.find_allChildren('span',{'class':\"job-criteria__text job-criteria__text--criteria\"},recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NavigableString' object has no attribute 'findChildren'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m test:\n\u001b[1;32m----> 3\u001b[0m     children \u001b[39m=\u001b[39m i\u001b[39m.\u001b[39;49mfindChildren(\u001b[39m\"\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m\"\u001b[39m,{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mjob-criteria__text job-criteria__text--criteria\u001b[39m\u001b[39m\"\u001b[39m},recursive\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m children:\n\u001b[0;32m      5\u001b[0m         i_want \u001b[39m=\u001b[39m child\u001b[39m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\TD_DATALAKE\\venv\\Lib\\site-packages\\bs4\\element.py:965\u001b[0m, in \u001b[0;36mNavigableString.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m    964\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    966\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[0;32m    967\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NavigableString' object has no attribute 'findChildren'"
     ]
    }
   ],
   "source": [
    "for i in test:\n",
    "\n",
    "    children = i.findChildren(\"span\",{'class':\"job-criteria__text job-criteria__text--criteria\"},recursive=True)\n",
    "    for child in children:\n",
    "        i_want = child.text\n",
    "        print(i_want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Niveau hiérarchique']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child = test.contents\n",
    "child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingénieur Expert(e) Talend (H/F) / Freelance\n"
     ]
    }
   ],
   "source": [
    "for title in soup.find_all('h1', attrs = {'class':'topcard__title'}): \n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niveau hiérarchique\n",
      "Type d’emploi\n",
      "Fonction\n",
      "Secteurs\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('h3', attrs = {'class':'job-criteria__subheader'}): \n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premier emploi\n",
      "Temps plein\n",
      "Ingénierie\n",
      "Technologies de l’information\n",
      "Technologies et services de\n",
      "                            l’information\n",
      "Logiciels\n",
      "                            informatiques\n",
      "Recrutement \n"
     ]
    }
   ],
   "source": [
    "for j in soup.find_all('span', attrs = {'class':'job-criteria__text job-criteria__text--criteria'}):\n",
    "    print(j.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issu du monde bancaire, CELAD a été créé en 1990\n",
      "                    à Toulouse. Nos 1350 collaborateurs(trices) interviennent aujourd’hui sur 2 pôles d’activités : les\n",
      "                    systèmes d’information et l’informatique industrielle et ceci chez plus de 250 clients (PME/PMI et\n",
      "                    grands comptes).Depuis 2006, l’entité CELAD Auvergne-Rhône-Alpes propose de nombreuses missions\n",
      "                    intéressantes et à forte valeur ajoutée sur l’ensemble de la région : Clermont-Ferrand, Montluçon,\n",
      "                    Lyon, Grenoble, St-Etienne, Valence, Dijon et Archamps.Nous sommes entièrement impliqué(e)s dans\n",
      "                    la gestion de nos collaborateurs(trices), ce qui est la base de notre développement ainsi que le\n",
      "                    reflet de la qualité de notre travail.Dotée d’un véritable processus de recrutement, l’agence\n",
      "                    CELAD Auvergne-Rhône-Alpes accompagne ses candidat(e)s avec beaucoup d’enthousiasme et de passion au\n",
      "                    quotidien.Donc si vous souhaitez faire partie de l’aventure Celadienne, rejoignez-nous\n",
      "                    !D’ailleurs, nous poursuivons notre développement et recrutons actuellement un(e) Ingénieur\n",
      "                    Expert(e) Talend pour intervenir chez un de nos clients.Contexte : Vous interviendrez sur divers\n",
      "                    sujets liés à la gestion des EDI du Domaine Décisionnel.Vos Principales Missions\n",
      "                            Quotidiennes\n",
      "\n",
      " La convergence sur une plate-forme technique unique de l’ensemble des flux EDI existants.\n",
      "                        \n",
      " La mise en uvre avec nos partenaires d’EDI disponibles et l’accompagnement de ce processus\n",
      "                            de bout en bout. Cet accompagnement intègre :\n",
      "la communication et l’explicitation de nos spécifications sur les formats de flux et sur les\n",
      "                    modalités techniques de communication sécurisée auprès des partenaires concernésla recette des\n",
      "                    flux produits par nos partenairesl’accompagnement de la mise en production et de l’intégration\n",
      "                    sous la plate-forme de surveillance et de communication.\n",
      "\n",
      " La participation potentielle à nos projets d’extension de l’offre de service EDI afin de\n",
      "                            couvrir de nouveaux besoins d’échange de données.\n",
      "Environnement Technique\n",
      "\n",
      " Expertise décisionnelle\n",
      " Expertise Talend\n",
      " Connaissance des métiers de l’Assurance serait un plus apprécié mais non indispensable\n",
      "\n",
      "Formation : Bac +5 et expérience de 5 ans minimum sur un poste similaire\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "for libelle_emp in soup.find_all('div', attrs = {\"description__text description__text--rich\"}) :\n",
    "    print(libelle_emp.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\n",
      "13546-INFO-EMP-LINKEDIN-FR-1599984246.html\n",
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\"\n",
    "list_dir_path = os.listdir(\"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\")\n",
    "list_file = [x for x in list_dir_path if x.endswith(\".html\")]\n",
    "list_file_info = [x for x in list_dir_path if x.endswith(\".html\") and x.__contains__(\"INFO\")]\n",
    "\n",
    "print(str(path))\n",
    "\n",
    "print(list_file_info[0])\n",
    "\n",
    "url = (str(path)+\"\\\\\"+str(list_file_info[0]))\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freelance-info.fr recrute pour des postes de Ingénieur Expert(e) Talend (H/F) / Freelance (Lyon, FR) | LinkedIn\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('title').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingénieur Expert(e) Talend (H/F) / Freelance (France)RechercherOffres d’emploiRéseauIgnorerIgnorerIgnorerIgnorer\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('section').get_text())\n",
    "\n",
    "#soup.prettify()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ba4306b892c702d24615d398cf592267eb03fb7bf2d599eb2ccdad9d12df570"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
