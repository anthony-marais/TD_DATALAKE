{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import platform\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import tqdm\n",
    "import re\n",
    "from python_function_datalake import MoveFileToLandingZone, GetOS,GenerateOSPathProject\n",
    "import click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linux'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os_system = GetOS()\n",
    "os_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUser():\n",
    "    user = getpass.getuser()\n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     File_ID                                   File_Path_Origin File_Size  \\\n",
       " 1          1  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  648.4 KB   \n",
       " 2          2  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  479.5 KB   \n",
       " 3          3  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  619.6 KB   \n",
       " 4          4  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  139.0 KB   \n",
       " 5          5  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  237.3 KB   \n",
       " ..       ...                                                ...       ...   \n",
       " 599      599  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  303.7 KB   \n",
       " 600      600  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  333.8 KB   \n",
       " 601      601  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  460.8 KB   \n",
       " 602      602  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  444.4 KB   \n",
       " 603      603  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  371.1 KB   \n",
       " \n",
       "                      File_Date  \\\n",
       " 1   2023-04-07 14:24:20.618968   \n",
       " 2   2023-04-07 14:24:20.619906   \n",
       " 3   2023-04-07 14:24:20.620995   \n",
       " 4   2023-04-07 14:24:20.621962   \n",
       " 5   2023-04-07 14:24:20.623046   \n",
       " ..                         ...   \n",
       " 599 2023-04-07 14:24:21.176356   \n",
       " 600 2023-04-07 14:24:21.177292   \n",
       " 601 2023-04-07 14:24:21.178071   \n",
       " 602 2023-04-07 14:24:21.178809   \n",
       " 603 2023-04-07 14:24:21.179598   \n",
       " \n",
       "                                  File_Path_Destination File_Destination_Size  \n",
       " 1    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              648.4 KB  \n",
       " 2    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              479.5 KB  \n",
       " 3    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              619.6 KB  \n",
       " 4    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              139.0 KB  \n",
       " 5    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              237.3 KB  \n",
       " ..                                                 ...                   ...  \n",
       " 599  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              303.7 KB  \n",
       " 600  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              333.8 KB  \n",
       " 601  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              460.8 KB  \n",
       " 602  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              444.4 KB  \n",
       " 603  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              371.1 KB  \n",
       " \n",
       " [603 rows x 6 columns],)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from python_function_datalake import MoveFileToLandingZone, GetOS,GenerateOSPathProject\n",
    "import click\n",
    "\n",
    "os_system = GetOS()\n",
    "path_project = GenerateOSPathProject(os_system)\n",
    "source_path = path_project[0]\n",
    "logfiles_path = path_project[1]\n",
    "landing_zone_linkedin_emp_path = path_project[2]\n",
    "landing_zone_glassdoor_soc_path = path_project[3]\n",
    "landing_zone_glassdoor_avi_path = path_project[4]\n",
    "linkedin_contains = path_project[5]\n",
    "glassdoor_soc_contains = path_project[6]\n",
    "glassdoor_avi_contains = path_project[7]\n",
    "path = path_project[8]\n",
    "endswith = path_project[9]\n",
    "contains_1 = path_project[10]\n",
    "contains_2 = path_project[11]\n",
    "delimiter_path = path_project[12]\n",
    "curated_zone_linkedin_emp_path = path_project[13]\n",
    "metadate_file_name_path = path_project[14]\n",
    "curated_zone_glassdoor_avis_path = path_project[15]\n",
    "\n",
    "\n",
    "#print(source_path)\n",
    "#print(logfiles_path)\n",
    "#print(landing_zone_linkedin_emp_path)\n",
    "#print(landing_zone_glassdoor_soc_path)\n",
    "#print(landing_zone_glassdoor_avi_path)\n",
    "#print(linkedin_contains)\n",
    "#print(glassdoor_soc_contains)\n",
    "#print(glassdoor_avi_contains)\n",
    "#print(path)\n",
    "#print(endswith)\n",
    "#print(contains_1)\n",
    "#print(contains_2)\n",
    "#print(delimiter_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "MoveFileToLandingZone(source_path, endswith, linkedin_contains, glassdoor_soc_contains, glassdoor_avi_contains,\n",
    "                      landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path, logfiles_path, delimiter_path,metadate_file_name_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_metadata_files = \"/home/virus/Documents/TD_DATALAKE/LOGFILES/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadMetadataFiles(metadate_file_name_path):\n",
    "    \"\"\"\n",
    "    Load metadata files\n",
    "    \"\"\"\n",
    "    metadata = pd.read_csv(metadate_file_name_path)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_ID</th>\n",
       "      <th>File_Path_Origin</th>\n",
       "      <th>File_Size</th>\n",
       "      <th>File_Date</th>\n",
       "      <th>File_Path_Destination</th>\n",
       "      <th>File_Destination_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/0...</td>\n",
       "      <td>648.4 KB</td>\n",
       "      <td>2023-04-07 14:24:20.618968</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>648.4 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/0...</td>\n",
       "      <td>479.5 KB</td>\n",
       "      <td>2023-04-07 14:24:20.619906</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>479.5 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/0...</td>\n",
       "      <td>619.6 KB</td>\n",
       "      <td>2023-04-07 14:24:20.620995</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>619.6 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/0...</td>\n",
       "      <td>139.0 KB</td>\n",
       "      <td>2023-04-07 14:24:20.621962</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>139.0 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/0...</td>\n",
       "      <td>237.3 KB</td>\n",
       "      <td>2023-04-07 14:24:20.623046</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>237.3 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>599</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/0...</td>\n",
       "      <td>303.7 KB</td>\n",
       "      <td>2023-04-07 14:24:21.176356</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>303.7 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>600</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/0...</td>\n",
       "      <td>333.8 KB</td>\n",
       "      <td>2023-04-07 14:24:21.177292</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>333.8 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>601</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/0...</td>\n",
       "      <td>460.8 KB</td>\n",
       "      <td>2023-04-07 14:24:21.178071</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>460.8 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>602</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/0...</td>\n",
       "      <td>444.4 KB</td>\n",
       "      <td>2023-04-07 14:24:21.178809</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>444.4 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>603</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/0...</td>\n",
       "      <td>371.1 KB</td>\n",
       "      <td>2023-04-07 14:24:21.179598</td>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>371.1 KB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_ID                                   File_Path_Origin File_Size  \\\n",
       "0          1  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  648.4 KB   \n",
       "1          2  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  479.5 KB   \n",
       "2          3  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  619.6 KB   \n",
       "3          4  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  139.0 KB   \n",
       "4          5  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  237.3 KB   \n",
       "..       ...                                                ...       ...   \n",
       "598      599  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  303.7 KB   \n",
       "599      600  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  333.8 KB   \n",
       "600      601  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  460.8 KB   \n",
       "601      602  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  444.4 KB   \n",
       "602      603  /home/anthony/Documents/TD_DATALAKE/DATALAKE/0...  371.1 KB   \n",
       "\n",
       "                      File_Date  \\\n",
       "0    2023-04-07 14:24:20.618968   \n",
       "1    2023-04-07 14:24:20.619906   \n",
       "2    2023-04-07 14:24:20.620995   \n",
       "3    2023-04-07 14:24:20.621962   \n",
       "4    2023-04-07 14:24:20.623046   \n",
       "..                          ...   \n",
       "598  2023-04-07 14:24:21.176356   \n",
       "599  2023-04-07 14:24:21.177292   \n",
       "600  2023-04-07 14:24:21.178071   \n",
       "601  2023-04-07 14:24:21.178809   \n",
       "602  2023-04-07 14:24:21.179598   \n",
       "\n",
       "                                 File_Path_Destination File_Destination_Size  \n",
       "0    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              648.4 KB  \n",
       "1    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              479.5 KB  \n",
       "2    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              619.6 KB  \n",
       "3    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              139.0 KB  \n",
       "4    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              237.3 KB  \n",
       "..                                                 ...                   ...  \n",
       "598  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              303.7 KB  \n",
       "599  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              333.8 KB  \n",
       "600  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              460.8 KB  \n",
       "601  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              444.4 KB  \n",
       "602  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...              371.1 KB  \n",
       "\n",
       "[603 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = LoadMetadataFiles(metadate_file_name_path)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDataLinkedin(metadate_file_name_path, curated_zone_linkedin_emp_path):\n",
    "    ''' Function to get data from html file where the file name contains a specific string (LINKEDIN) '''\n",
    "\n",
    "    \"\"\"\n",
    "    Call LoadMetadataFiles function\n",
    "    \"\"\"\n",
    "    landing_zone_path = LoadMetadataFiles(metadate_file_name_path)\n",
    "\n",
    "    linkedin_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\n",
    "        \"LINKEDIN\")]\n",
    "    len_linkedin_files = len(linkedin_files)\n",
    "\n",
    "    print(\"Number of Linkedin files: \", len_linkedin_files)\n",
    "\n",
    "    list_of_data = []\n",
    "    title_file = []\n",
    "    columns_dataframe = [\"file\", \"title\", \"society\", \"city\", \"description\"]\n",
    "    data_title = []\n",
    "    data_society = []\n",
    "    data_city = []\n",
    "    data_country = []\n",
    "    data_description = []\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for i in linkedin_files[\"File_Path_Destination\"]:\n",
    "        title_file.append(i)\n",
    "\n",
    "        with open(i, 'r', encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            soup = bs(f, 'html.parser')\n",
    "            data_title.append(soup.find_all('h1', attrs={'class': 'topcard__title'})[0].text)\n",
    "            data_society.append(soup.find_all('span', attrs={'class': 'topcard__flavor'})[0].text)\n",
    "            data_city.append(soup.find_all('span', attrs={'class': 'topcard__flavor topcard__flavor--bullet'})[0].text)\n",
    "            data_description.append(soup.find_all('div', attrs={'class': 'description__text description__text--rich'})[0].text)\n",
    "\n",
    "    print(\"Number of Linkedin files process: \", len_linkedin_files, \"\\n\")\n",
    "    data_linkedin = pd.DataFrame(list(zip(\n",
    "        title_file, data_title, data_society, data_city, data_description)), columns=columns_dataframe)\n",
    "    data_linkedin.index = np.arange(1, len(data_linkedin) + 1)\n",
    "    \n",
    "    return data_linkedin\n",
    "    \n",
    "    #data_linkedin.to_csv(curated_zone_linkedin_emp_path +\n",
    "    #                     \"/data_linkedin.csv\", index=True, encoding=\"utf-8\", header=True)\n",
    "    #print(\"Dataframe Linkedin saved in: \",\n",
    "    #      curated_zone_linkedin_emp_path+\"/data_linkedin.csv\")\n",
    "    #print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Linkedin files:  254\n",
      "\n",
      "\n",
      "Number of Linkedin files process:  254 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>title</th>\n",
       "      <th>society</th>\n",
       "      <th>city</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Big Data Engineer</td>\n",
       "      <td>Aubay France</td>\n",
       "      <td>Région de Paris, France</td>\n",
       "      <td>La data n’a plus de secret pour toi ? En tant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Data analyst</td>\n",
       "      <td>DTA INGENIERIE MEDITERRANEE</td>\n",
       "      <td>Lyon 03, FR</td>\n",
       "      <td>Cette offre d’emploi est fournie par Pôle empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Biostatistician / Data Scientist (M/F)</td>\n",
       "      <td>Bayer</td>\n",
       "      <td>Dargoire, FR</td>\n",
       "      <td>Bayer is a global enterprise with core compete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Ingénieur(e) décisionnel - BusinessIntelligence</td>\n",
       "      <td>AVISTO</td>\n",
       "      <td>Aix-en-Provence, FR</td>\n",
       "      <td>A Propos De L'entrepriseRejoindre AViSTO, c'es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Ingénieur sécurité infrastructure H/F</td>\n",
       "      <td>Devoteam</td>\n",
       "      <td>Lyon, FR</td>\n",
       "      <td>Description de l'entrepriseChez Devoteam, nous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Data Analyst F/H</td>\n",
       "      <td>INTITEK</td>\n",
       "      <td>Lyon, FR</td>\n",
       "      <td>Description Quoi de plus stimulant que de part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Consultant décisionnel Informatica H/F</td>\n",
       "      <td>Gfi world</td>\n",
       "      <td>Saint-Ouen, FR</td>\n",
       "      <td>Détail de l'offre  Informations générales  Ent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Awalee Consulting</td>\n",
       "      <td>Région de Paris, France</td>\n",
       "      <td>Vous êtes Data Scientist, passionné(e) par les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Ingénieur étude et développement BIG DATA (H/F)</td>\n",
       "      <td>Astek</td>\n",
       "      <td>Paris, FR</td>\n",
       "      <td>Le département SI du groupe ASTEK, recherche u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Ingénieur Décisionnel/BI H/F</td>\n",
       "      <td>EOLE Consulting</td>\n",
       "      <td>Lyon, FR</td>\n",
       "      <td>Poste et missions Nous recherchons pour le com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  file  \\\n",
       "1    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "2    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "3    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "4    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "5    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "..                                                 ...   \n",
       "250  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "251  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "252  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "253  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "254  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "\n",
       "                                               title  \\\n",
       "1                                  Big Data Engineer   \n",
       "2                                       Data analyst   \n",
       "3             Biostatistician / Data Scientist (M/F)   \n",
       "4    Ingénieur(e) décisionnel - BusinessIntelligence   \n",
       "5              Ingénieur sécurité infrastructure H/F   \n",
       "..                                               ...   \n",
       "250                                 Data Analyst F/H   \n",
       "251           Consultant décisionnel Informatica H/F   \n",
       "252                                   Data Scientist   \n",
       "253  Ingénieur étude et développement BIG DATA (H/F)   \n",
       "254                     Ingénieur Décisionnel/BI H/F   \n",
       "\n",
       "                         society                     city  \\\n",
       "1                   Aubay France  Région de Paris, France   \n",
       "2    DTA INGENIERIE MEDITERRANEE              Lyon 03, FR   \n",
       "3                          Bayer             Dargoire, FR   \n",
       "4                         AVISTO      Aix-en-Provence, FR   \n",
       "5                       Devoteam                 Lyon, FR   \n",
       "..                           ...                      ...   \n",
       "250                      INTITEK                 Lyon, FR   \n",
       "251                    Gfi world           Saint-Ouen, FR   \n",
       "252            Awalee Consulting  Région de Paris, France   \n",
       "253                        Astek                Paris, FR   \n",
       "254              EOLE Consulting                 Lyon, FR   \n",
       "\n",
       "                                           description  \n",
       "1    La data n’a plus de secret pour toi ? En tant ...  \n",
       "2    Cette offre d’emploi est fournie par Pôle empl...  \n",
       "3    Bayer is a global enterprise with core compete...  \n",
       "4    A Propos De L'entrepriseRejoindre AViSTO, c'es...  \n",
       "5    Description de l'entrepriseChez Devoteam, nous...  \n",
       "..                                                 ...  \n",
       "250  Description Quoi de plus stimulant que de part...  \n",
       "251  Détail de l'offre  Informations générales  Ent...  \n",
       "252  Vous êtes Data Scientist, passionné(e) par les...  \n",
       "253  Le département SI du groupe ASTEK, recherche u...  \n",
       "254  Poste et missions Nous recherchons pour le com...  \n",
       "\n",
       "[254 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetDataLinkedin(metadate_file_name_path, curated_zone_linkedin_emp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDataGlassdoorAvis(metadate_file_name_path, curated_zone_glassdoor_avis_path):\n",
    "    ''' Function to get data from html file where the file name contains a specific string (AVIS-SOC) '''\n",
    "\n",
    "    landing_zone_path = LoadMetadataFiles(metadate_file_name_path)\n",
    "    glassdoor_avis_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\n",
    "        \"AVIS-SOC\")]\n",
    "    len_glassdoor_avis_files = len(glassdoor_avis_files)\n",
    "    print(\"Number of Glassdoor avis files: \", len_glassdoor_avis_files)\n",
    "\n",
    "    list_data_avis = []\n",
    "    columns_dataframe_avis = [\"file\", \"society\", \"date\", \"avis_positif\",\n",
    "                              \"avis_negatif\", \"status\", \"poste\", \"localisation\", \"mean_rate\", \"general_rate\"]\n",
    "    title_avis = []\n",
    "    avis_society = []\n",
    "    date_avis_glassdoor = []\n",
    "    avis_positif_list = []\n",
    "    avis_negatif_list = []\n",
    "    status_list = []\n",
    "    poste_list = []\n",
    "    localisation_list = []\n",
    "    mean_rate_list = []\n",
    "    rate_list = []\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for i in glassdoor_avis_files[\"File_Path_Destination\"]:\n",
    "        title_avis.append(i)\n",
    "        with open(i, 'r', encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            soup = bs(f, 'html.parser')\n",
    "            list_data_avis.append([i for i in soup.find_all(\n",
    "                'p', attrs={\"class\": \"h1 strong tightAll\"})][0].text)\n",
    "            date_avis_glassdoor.append([i for i in soup.find_all(\n",
    "                'time', attrs={'class': 'date subtle small'})][0].text)\n",
    "            avis_positif_list.append([i for i in soup.find(string='Avantages').findNext('p')][0].text if len([i for i in soup.find(string='Avantages').findNext('p')]) > 0 else \"None\")\n",
    "            avis_negatif_list.append([i for i in soup.find(string='Inconvénients').findNext('p')][0].text if len([i for i in soup.find(string='Avantages').findNext('p')]) > 0 else \"None\")\n",
    "            status_list.append([i for i in soup.find_all('span', attrs={\"class\": \"authorJobTitle middle reviewer\"})][0].text.split(\"-\")[0].strip())\n",
    "            poste_list.append([i for i in soup.find_all('span', attrs={\"class\": \"authorJobTitle middle reviewer\"})][0].text.split(\"-\")[1].strip() if len([i for i in soup.find_all('span', attrs={\"class\": \"authorJobTitle middle reviewer\"})][0].text.split(\"-\")) > 1 else \"None\")\n",
    "            localisation_list.append([i for i in soup.find_all('span', attrs={\"class\": \"authorLocation\"})][0].text if len([i for i in soup.find_all('span', attrs={\"class\": \"authorLocation\"})]) > 0 else \"None\")\n",
    "            mean_rate_list.append([i for i in soup.find_all('div', attrs={\"class\": \"v2__EIReviewsRatingsStylesV2__ratingNum v2__EIReviewsRatingsStylesV2__large\"})][0].text)\n",
    "            rate_list.append(re.sub(\n",
    "                r'<span class=\"(.*)\" title=\"(.*)\">(.*)</span>(.*)', r'\\2', str(soup.find_all('span', attrs={\n",
    "                                       \"class\": \"gdStars gdRatings sm mr-sm mr-md-std stars__StarsStyles__gdStars\"})[0].span.contents[0])))\n",
    "\n",
    "    print(\"Number of Glassdoor avis files process: \",\n",
    "          len_glassdoor_avis_files, \"\\n\")\n",
    "    data_glassdoor_avis = pd.DataFrame(list(zip(title_avis, list_data_avis, date_avis_glassdoor, avis_positif_list,\n",
    "                                       avis_negatif_list, status_list, poste_list, localisation_list, mean_rate_list, rate_list)), columns=columns_dataframe_avis)\n",
    "    data_glassdoor_avis.index = np.arange(1, len(data_glassdoor_avis) + 1)\n",
    "    \n",
    "    return data_glassdoor_avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Glassdoor avis files:  209\n",
      "\n",
      "\n",
      "Number of Glassdoor avis files process:  209 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>society</th>\n",
       "      <th>date</th>\n",
       "      <th>avis_positif</th>\n",
       "      <th>avis_negatif</th>\n",
       "      <th>status</th>\n",
       "      <th>poste</th>\n",
       "      <th>localisation</th>\n",
       "      <th>mean_rate</th>\n",
       "      <th>general_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Groupe Adéquat</td>\n",
       "      <td>Nov 24, 2017</td>\n",
       "      <td>Valeurs humaines et respect des individus, peu...</td>\n",
       "      <td>Mutuelle beaucoup trop chère -&gt; environ 50€/sa...</td>\n",
       "      <td>Employé actuel</td>\n",
       "      <td>Chargée De Recrutement</td>\n",
       "      <td>Chaumont, Champagne-Ardenne</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>GFI Software</td>\n",
       "      <td>Apr 22, 2016</td>\n",
       "      <td>La mutuelle et le CE me paraissaient relativem...</td>\n",
       "      <td>GFI ressemble à beaucoup d'autres SSII importa...</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>Lyon</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>CELINE</td>\n",
       "      <td>Feb 20, 2018</td>\n",
       "      <td>Tres bonne ambiance et esprit d'equpe,  des co...</td>\n",
       "      <td>Comme c'est le cas souvent dans les maisons de...</td>\n",
       "      <td>Ancien employé</td>\n",
       "      <td>Employé anonyme</td>\n",
       "      <td>Paris</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Groupe Adéquat</td>\n",
       "      <td>Nov 24, 2017</td>\n",
       "      <td>Valeurs humaines et respect des individus, peu...</td>\n",
       "      <td>Mutuelle beaucoup trop chère -&gt; environ 50€/sa...</td>\n",
       "      <td>Employé actuel</td>\n",
       "      <td>Chargée De Recrutement</td>\n",
       "      <td>Chaumont, Champagne-Ardenne</td>\n",
       "      <td>3.9</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Artefact</td>\n",
       "      <td>Feb 5, 2019</td>\n",
       "      <td>Cadre, Ambiance,  salaire, Evolution</td>\n",
       "      <td>Rien à dire. Pas d'inconvénients ...</td>\n",
       "      <td>Ancien employé</td>\n",
       "      <td>Employé anonyme</td>\n",
       "      <td>Paris</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Artefacts Studio</td>\n",
       "      <td>May 22, 2017</td>\n",
       "      <td>Expérience personnellement enrichissante de pa...</td>\n",
       "      <td>Un studio qui gagne à être davantage connu</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>STM</td>\n",
       "      <td>Dec 10, 2019</td>\n",
       "      <td>Horaire flexible, salaires, Équipe, diversité ...</td>\n",
       "      <td>les processus interne peuvent être amèliorer</td>\n",
       "      <td>Ancien employé</td>\n",
       "      <td>Employé anonyme</td>\n",
       "      <td>Montréal, QC</td>\n",
       "      <td>3.7</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>CEGID</td>\n",
       "      <td>Dec 4, 2019</td>\n",
       "      <td>Bonne ambiance, rémunération attractive entre ...</td>\n",
       "      <td>Rigidité de l’organisation, des financiers tou...</td>\n",
       "      <td>Employé actuel</td>\n",
       "      <td>Senior Account Manager</td>\n",
       "      <td>Paris</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>ITS Overlap</td>\n",
       "      <td>May 10, 2018</td>\n",
       "      <td>bonne ambiance avec les collègues</td>\n",
       "      <td>manque de ressources\\ntrop axé pratique de SSII</td>\n",
       "      <td>Employé actuel</td>\n",
       "      <td>Employé anonyme</td>\n",
       "      <td>Boulogne-Billancourt</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Sopra Steria Recruitment</td>\n",
       "      <td>Oct 16, 2017</td>\n",
       "      <td>Ambiance conviviale\\nAvantages Mutuelle, CE\\nP...</td>\n",
       "      <td>Salaire\\nNon augmentation\\nDiscrimination au n...</td>\n",
       "      <td>Employé actuel</td>\n",
       "      <td>Employé anonyme</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>209 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  file  \\\n",
       "1    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "2    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "3    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "4    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "5    /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "..                                                 ...   \n",
       "205  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "206  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "207  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "208  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "209  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "\n",
       "                      society          date  \\\n",
       "1              Groupe Adéquat  Nov 24, 2017   \n",
       "2                GFI Software  Apr 22, 2016   \n",
       "3                      CELINE  Feb 20, 2018   \n",
       "4              Groupe Adéquat  Nov 24, 2017   \n",
       "5                    Artefact   Feb 5, 2019   \n",
       "..                        ...           ...   \n",
       "205          Artefacts Studio  May 22, 2017   \n",
       "206                       STM  Dec 10, 2019   \n",
       "207                     CEGID   Dec 4, 2019   \n",
       "208               ITS Overlap  May 10, 2018   \n",
       "209  Sopra Steria Recruitment  Oct 16, 2017   \n",
       "\n",
       "                                          avis_positif  \\\n",
       "1    Valeurs humaines et respect des individus, peu...   \n",
       "2    La mutuelle et le CE me paraissaient relativem...   \n",
       "3    Tres bonne ambiance et esprit d'equpe,  des co...   \n",
       "4    Valeurs humaines et respect des individus, peu...   \n",
       "5                 Cadre, Ambiance,  salaire, Evolution   \n",
       "..                                                 ...   \n",
       "205  Expérience personnellement enrichissante de pa...   \n",
       "206  Horaire flexible, salaires, Équipe, diversité ...   \n",
       "207  Bonne ambiance, rémunération attractive entre ...   \n",
       "208                  bonne ambiance avec les collègues   \n",
       "209  Ambiance conviviale\\nAvantages Mutuelle, CE\\nP...   \n",
       "\n",
       "                                          avis_negatif          status  \\\n",
       "1    Mutuelle beaucoup trop chère -> environ 50€/sa...  Employé actuel   \n",
       "2    GFI ressemble à beaucoup d'autres SSII importa...                   \n",
       "3    Comme c'est le cas souvent dans les maisons de...  Ancien employé   \n",
       "4    Mutuelle beaucoup trop chère -> environ 50€/sa...  Employé actuel   \n",
       "5                 Rien à dire. Pas d'inconvénients ...  Ancien employé   \n",
       "..                                                 ...             ...   \n",
       "205         Un studio qui gagne à être davantage connu                   \n",
       "206       les processus interne peuvent être amèliorer  Ancien employé   \n",
       "207  Rigidité de l’organisation, des financiers tou...  Employé actuel   \n",
       "208    manque de ressources\\ntrop axé pratique de SSII  Employé actuel   \n",
       "209  Salaire\\nNon augmentation\\nDiscrimination au n...  Employé actuel   \n",
       "\n",
       "                      poste                 localisation mean_rate  \\\n",
       "1    Chargée De Recrutement  Chaumont, Champagne-Ardenne       3.9   \n",
       "2                      None                         Lyon       2.2   \n",
       "3           Employé anonyme                        Paris       3.0   \n",
       "4    Chargée De Recrutement  Chaumont, Champagne-Ardenne       3.9   \n",
       "5           Employé anonyme                        Paris       4.0   \n",
       "..                      ...                          ...       ...   \n",
       "205                    None                         None       4.6   \n",
       "206         Employé anonyme                 Montréal, QC       3.7   \n",
       "207  Senior Account Manager                        Paris       2.7   \n",
       "208         Employé anonyme         Boulogne-Billancourt       3.0   \n",
       "209         Employé anonyme                         None       4.0   \n",
       "\n",
       "    general_rate  \n",
       "1            5.0  \n",
       "2            2.0  \n",
       "3            5.0  \n",
       "4            5.0  \n",
       "5            4.0  \n",
       "..           ...  \n",
       "205          5.0  \n",
       "206          4.0  \n",
       "207          5.0  \n",
       "208          3.0  \n",
       "209          4.0  \n",
       "\n",
       "[209 rows x 10 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetDataGlassdoorAvis(metadate_file_name_path, curated_zone_glassdoor_avis_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# , curated_zone_glassdoor_soc_path):\n",
    "def GetDataGlassdoorSociety(metadate_file_name_path):\n",
    "    ''' Function to get data from html file where the file name contains a specific string (INFO) AND (LINKEDIN) '''\n",
    "\n",
    "    \"\"\"\n",
    "    Call LoadMetadataFiles function\n",
    "    \"\"\"\n",
    "    landing_zone_path = LoadMetadataFiles(metadate_file_name_path)\n",
    "    glassdoor_soc_files = landing_zone_path[landing_zone_path[\"File_Path_Destination\"].str.contains(\n",
    "        \"INFO-SOC\")]\n",
    "    len_glassdoor_soc_files = len(glassdoor_soc_files)\n",
    "\n",
    "    print(\"Number of Glassdoor soc files: \", len_glassdoor_soc_files)\n",
    "    glassdoor_society_name_list = []\n",
    "    title_glassdoor_soc = []\n",
    "    glassdoor_soc_city_list = []\n",
    "    glassdoor_soc_size_list = []\n",
    "    glassdoor_secteur_list = []\n",
    "    glassdoor_fondation_list = []\n",
    "    glassdoor_soc_website_list = []\n",
    "    columns_glassdoor_soc = [\"file\", \"society\",\n",
    "                             \"city\", \"size\", \"secteur\", \"fondation\", \"website\"]\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for i in glassdoor_soc_files[\"File_Path_Destination\"]:\n",
    "        title_glassdoor_soc.append(i)\n",
    "        with open(i, 'r', encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            soup = bs(f, 'html.parser')\n",
    "\n",
    "            glassdoor_society_name = [i for i in soup.find_all(\n",
    "                'h1', attrs={'class': \"strong tightAll\"})[0]]\n",
    "            glassdoor_society_name_list.append(\n",
    "                glassdoor_society_name[0].string)\n",
    "            glassdoor_soc_city_list.append(soup.find_all(\n",
    "                'div', attrs={'class': \"infoEntity\"})[1].span.string)\n",
    "            glassdoor_soc_size_list.append(soup.find_all(\n",
    "                'div', attrs={'class': \"infoEntity\"})[2].span.string)\n",
    "            # .findNext('span').text)#.findNext('span').text)\n",
    "            glassdoor_secteur_list.append(\n",
    "                soup.find(string='Secteur').findNext('span').string if len(soup.find(string='Secteur').findNext('span').string) > 0 else \"None\")\n",
    "\n",
    "            glassdoor_fondation_list.append(soup.find(string=['Fondé en', 'Créé en dans les années', 'existe depuis']).findNext('span').string\n",
    "                                            if len(soup.find(string=['Fondé en', 'Créé en dans les années', 'existe depuis']).findNext('span').string) > 0 else \"None\")\n",
    "\n",
    "            glassdoor_soc_website_list.append(re.sub(r'(.*)<h1 class=\" strong tightAll\" data-company=\"(.*)\" title=\"\">(.*)', r'\\2', str(soup.find_all('div', attrs={'class': \"infoEntity\"})[0].string)) if len(soup.find_all('div', attrs={'class': \"infoEntity\"})) > 0 else \"None\")\n",
    "\n",
    "    data_glassdoor_soc = pd.DataFrame(list(zip(title_glassdoor_soc, glassdoor_society_name_list, glassdoor_soc_city_list,\n",
    "                                      glassdoor_soc_size_list, glassdoor_secteur_list, glassdoor_fondation_list, glassdoor_soc_website_list)), columns=columns_glassdoor_soc)\n",
    "\n",
    "    return data_glassdoor_soc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Glassdoor soc files:  140\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g_s = GetDataGlassdoorSociety(metadate_file_name_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>society</th>\n",
       "      <th>city</th>\n",
       "      <th>size</th>\n",
       "      <th>secteur</th>\n",
       "      <th>fondation</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Dassault Systemes</td>\n",
       "      <td>Vélizy</td>\n",
       "      <td>Plus de 10 000 employés</td>\n",
       "      <td>Matériel et logiciels informatiques</td>\n",
       "      <td>1981</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>ALTRAD Group</td>\n",
       "      <td>Montpellier</td>\n",
       "      <td>De 1 001 à 5 000 employés</td>\n",
       "      <td>Inconnu</td>\n",
       "      <td>Inconnu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Mixt Composites Recyclables</td>\n",
       "      <td>Tournon-sur-Rhône</td>\n",
       "      <td>Inconnu</td>\n",
       "      <td>Inconnu</td>\n",
       "      <td>Inconnu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Capgemini</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Plus de 10 000 employés</td>\n",
       "      <td>Solutions logicielles et réseau d'entreprise</td>\n",
       "      <td>1967</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Fortuneo SA</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Entre 201 et 500 employés</td>\n",
       "      <td>Banques et organismes de crédit</td>\n",
       "      <td>2000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>D2SI</td>\n",
       "      <td>Paris</td>\n",
       "      <td>De 51 à 200 employés</td>\n",
       "      <td>Conseils</td>\n",
       "      <td>2006</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Capgemini Invent</td>\n",
       "      <td>Courbevoie</td>\n",
       "      <td>De 5 001 à 10 000 employés</td>\n",
       "      <td>Conseils</td>\n",
       "      <td>2009</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>CELINE</td>\n",
       "      <td>Paris</td>\n",
       "      <td>De 1 001 à 5 000 employés</td>\n",
       "      <td>Magasins de chaussures, de vêtements et grand...</td>\n",
       "      <td>1945</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>CEGID</td>\n",
       "      <td>Lyon</td>\n",
       "      <td>De 1 001 à 5 000 employés</td>\n",
       "      <td>Solutions logicielles et réseau d'entreprise</td>\n",
       "      <td>1983</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/home/anthony/Documents/TD_DATALAKE/DATALAKE/1...</td>\n",
       "      <td>Veolia</td>\n",
       "      <td>Paris</td>\n",
       "      <td>Plus de 10 000 employés</td>\n",
       "      <td>Services publics de distribution</td>\n",
       "      <td>1853</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  \\\n",
       "0  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "1  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "2  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "3  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "4  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "5  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "6  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "7  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "8  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "9  /home/anthony/Documents/TD_DATALAKE/DATALAKE/1...   \n",
       "\n",
       "                       society               city                        size  \\\n",
       "0            Dassault Systemes             Vélizy     Plus de 10 000 employés   \n",
       "1                 ALTRAD Group        Montpellier   De 1 001 à 5 000 employés   \n",
       "2  Mixt Composites Recyclables  Tournon-sur-Rhône                     Inconnu   \n",
       "3                    Capgemini              Paris     Plus de 10 000 employés   \n",
       "4                  Fortuneo SA              Paris   Entre 201 et 500 employés   \n",
       "5                         D2SI              Paris        De 51 à 200 employés   \n",
       "6             Capgemini Invent         Courbevoie  De 5 001 à 10 000 employés   \n",
       "7                       CELINE              Paris   De 1 001 à 5 000 employés   \n",
       "8                        CEGID               Lyon   De 1 001 à 5 000 employés   \n",
       "9                       Veolia              Paris     Plus de 10 000 employés   \n",
       "\n",
       "                                             secteur fondation website  \n",
       "0                Matériel et logiciels informatiques      1981    None  \n",
       "1                                            Inconnu   Inconnu    None  \n",
       "2                                            Inconnu   Inconnu    None  \n",
       "3       Solutions logicielles et réseau d'entreprise      1967    None  \n",
       "4                    Banques et organismes de crédit      2000    None  \n",
       "5                                           Conseils      2006    None  \n",
       "6                                           Conseils      2009    None  \n",
       "7   Magasins de chaussures, de vêtements et grand...      1945    None  \n",
       "8       Solutions logicielles et réseau d'entreprise      1983    None  \n",
       "9                   Services publics de distribution      1853    None  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_s.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glassdoor_city_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m glassdoor_city_list\u001b[39m.\u001b[39mappend(soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, attrs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39minfoEntity\u001b[39m\u001b[39m\"\u001b[39m})[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mspan\u001b[39m.\u001b[39mtext)\n\u001b[1;32m      3\u001b[0m glassdoor_society_name \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mh1\u001b[39m\u001b[39m'\u001b[39m, attrs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mstrong tightAll\u001b[39m\u001b[39m\"\u001b[39m})[\u001b[39m0\u001b[39m]]\n\u001b[1;32m      4\u001b[0m glassdoor_society_name_list\u001b[39m.\u001b[39mappend(glassdoor_society_name[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glassdoor_city_list' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "            glassdoor_city_list.append(soup.find_all('div', attrs={'class': \"infoEntity\"})[1].span.text)\n",
    "\n",
    "            glassdoor_society_name = [i for i in soup.find_all('h1', attrs={'class': \"strong tightAll\"})[0]]\n",
    "            glassdoor_society_name_list.append(glassdoor_society_name[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"module empBasicInfo noPadBot\" data-emp-id=\"9028\" id=\"EmpBasicInfo\"><div class=\"\"><header class=\"tbl fill\"><h2 class=\"cell middle tightVert blockMob\"> Présentation de Gfi Informatique</h2></header><div class=\"info flexbox row col-hh\"><div class=\"infoEntity\"><label>Site Web</label><span class=\"value website\"><a class=\"link\" href=\"http://www.gfi.world\" rel=\"nofollow noreferrer\" target=\"_blank\">www.gfi.world</a></span></div><div class=\"infoEntity\"><label>Siège social</label><span class=\"value\">Saint-Ouen, Île-de-France</span></div><div class=\"infoEntity\"><label>Taille</label><span class=\"value\">Plus de 10 000 employés</span></div><div class=\"infoEntity\"><label>Fondé en</label><span class=\"value\"> 1995</span></div><div class=\"infoEntity\"><label>Type</label><span class=\"value\"> Entreprise cotée en bourse</span></div><div class=\"infoEntity\"><label>Secteur</label><span class=\"value\"> Services informatiques</span></div><div class=\"infoEntity\"><label>Revenu</label><span class=\"value\"> Entre 1 et 2 milliards € (EUR) par an</span></div></div></div><div class=\"gdGrid\"><div class=\"d-flex justify-content-start py-std flex-wrap flex-md-nowrap\"><div class=\"strong label\"> Concurrents</div><div class=\"pb-std pb-md-0\"><p class=\"m-0\"> Sopra Steria, Capgemini, ALTEN</p></div></div></div><div class=\"\"><div class=\"margTop empDescription\" data-full=\"&amp;Agrave; propos de Gfi&lt;br/&gt;Le Groupe Gfi, pr&amp;eacute;sent dans plus de 20 pays, est un leader incontournable des services informatiques &amp;agrave; valeur ajout&amp;eacute;e et des logiciels. Gfi occupe un positionnement strat&amp;eacute;gique diff&amp;eacute;renciant entre les op&amp;eacute;rateurs de taille mondiale et les acteurs de niche. Avec son profil de multi-sp&amp;eacute;cialiste, Gfi met au service de ses clients une combinaison unique de proximit&amp;eacute;, d&amp;rsquo;organisation sectorielle et de solutions de qualit&amp;eacute; industrielle. Le Groupe qui compte pr&amp;egrave;s de 19 500 collaborateurs a r&amp;eacute;alis&amp;eacute; en 2018 un chiffre d&amp;rsquo;affaires de 1 395 M&amp;euro;.&lt;br/&gt;Pour plus d&amp;rsquo;informations : www.gfi.world\"> À propos de Gfi<br/>Le Groupe Gfi, présent dans plus de 20 pays, est un leader incontournable des services informatiques à valeur ajoutée et des logiciels. Gfi occupe un positionnement stratégique ... <span class=\"link minor moreLink\" id=\"ExpandDesc\">En savoir plus</span></div><div class=\"margTopMd\"><div id=\"InlineLoginModule\"></div></div></div></div>]\n",
      "<div id=\"InlineLoginModule\"></div>\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/anthony/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/SOC/13778-INFO-SOC-GLASSDOOR-E9028_P1.html\", 'r', encoding=\"utf-8\",errors=\"replace\") as f:\n",
    "#    soup = bs(f, 'html.parser')\n",
    "    soup = bs(f, 'html.parser')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    myTest = soup.find_all('div', attrs={'id': \"EmpBasicInfo\"})\n",
    "    print(myTest)\n",
    "    myTest2 = re.sub(r'(.*)data-full=\"(.*).<br/>(.*)', r'\\2', str(myTest[0].find_all('div', attrs={'class': \"\"})[2]))\n",
    "    print(myTest2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = \"\"\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "list = []\n",
    "\n",
    "\n",
    "for combination in itertools.product(range(10), repeat=6):\n",
    "    \n",
    "    list.append(''.join(map(str, combination)))\n",
    "\n",
    "\n",
    "pd.DataFrame(list).to_csv(\"/home/virus/Documents/phone_number.csv\", index=True, encoding=\"utf-8\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'163450'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list[163450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linux'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def GetOS():\n",
    "    system = platform.system()\n",
    "    return system\n",
    "\n",
    "os_system = GetOS()\n",
    "os_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linux'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = platform.system()\n",
    "system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUser():\n",
    "    user = getpass.getuser()\n",
    "    return user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateOSPathProject(os_system):\n",
    "\n",
    "    load_dotenv()\n",
    "    \n",
    "    if os_system != \"Windows\":\n",
    "        user = GetUser()\n",
    "        source_path = os.environ.get('source_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        logfiles_path = os.environ.get('logfiles_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        landing_zone_linkedin_emp_path = os.environ.get(\n",
    "            'landing_zone_linkedin_emp_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        landing_zone_glassdoor_soc_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_soc_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        landing_zone_glassdoor_avi_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_avi_path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        linkedin_contains = os.environ.get('linkedin_contains')\n",
    "        glassdoor_soc_contains = os.environ.get('glassdoor_soc_contains')\n",
    "        glassdoor_avi_contains = os.environ.get('glassdoor_avi_contains')\n",
    "        path = os.environ.get('path_linux').replace(\"//\", \"/\"+user+\"/\")\n",
    "        endswith = os.environ.get('endswith')\n",
    "        contains_1 = os.environ.get('contains_1')\n",
    "        contains_2 = os.environ.get('contains_2')\n",
    "        delimiter_path = \"/\"\n",
    "\n",
    "    elif os_system == \"Windows\":\n",
    "        source_path = os.environ.get('source_path_windows')\n",
    "        logfiles_path = os.environ.get('logfiles_path_windows')\n",
    "        landing_zone_linkedin_emp_path = os.environ.get(\n",
    "            'landing_zone_linkedin_emp_path_windows')\n",
    "        landing_zone_glassdoor_soc_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_soc_path_windows')\n",
    "        landing_zone_glassdoor_avi_path = os.environ.get(\n",
    "            'landing_zone_glassdoor_avi_path_windows')\n",
    "        linkedin_contains = os.environ.get('linkedin_contains')\n",
    "        glassdoor_soc_contains = os.environ.get('glassdoor_soc_contains')\n",
    "        glassdoor_avi_contains = os.environ.get('glassdoor_avi_contains')\n",
    "        path = os.environ.get('path_windows')\n",
    "        endswith = os.environ.get('endswith')\n",
    "        contains_1 = os.environ.get('contains_1')\n",
    "        contains_2 = os.environ.get('contains_2')\n",
    "        delimiter_path = \"\\\\\"\n",
    "\n",
    "\n",
    "    return source_path, logfiles_path, landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path, linkedin_contains, glassdoor_soc_contains, glassdoor_avi_contains, path, endswith, contains_1, contains_2, delimiter_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'source_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os_path_project \u001b[39m=\u001b[39m GenerateOSPathProject(os_system)\u001b[39m.\u001b[39;49msource_path\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'source_path'"
     ]
    }
   ],
   "source": [
    "os_path_project = GenerateOSPathProject(os_system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'source_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m source_path\n",
      "\u001b[0;31mNameError\u001b[0m: name 'source_path' is not defined"
     ]
    }
   ],
   "source": [
    "source_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPath(os_path_project):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateWotkingPath(source_path,user):\n",
    "    working_path = source_path.replace(\"//\", \"/\"+user+\"/\")\n",
    "    return working_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GenerateWotkingPath(source_path,user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetUser():\n",
    "    user = getpass.getuser()\n",
    "    return user\n",
    "\n",
    "def GetOS():\n",
    "    system = platform.system()\n",
    "    if system == \"/\":\n",
    "        return \"Linux\"\n",
    "    elif system == \"Windows\":\n",
    "        return \"\\\\\"\n",
    "    else:\n",
    "        return \"/\"\n",
    "\n",
    "def DefinePathOS(path):\n",
    "    delimiter_path = GetOS()\n",
    "    if delimiter_path == \"Linux\":\n",
    "        return path.replace(\"\\\\\", \"/\")\n",
    "    elif delimiter_path == \"Windows\":\n",
    "        return path.replace(\"/\", \"\\\\\")\n",
    "    else:\n",
    "        return path.replace(\"\\\\\", \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetOS():\n",
    "    system = platform.system()\n",
    "    if system == \"/\":\n",
    "        return \"Linux\"\n",
    "    elif system == \"Windows\":\n",
    "        return \"\\\\\"\n",
    "    else:\n",
    "        return \"/\"\n",
    "\n",
    "''' Function to count number of file in a directory '''\n",
    "def GetFilesPath(path,endswith,delimiter_path):\n",
    "    count = 0\n",
    "    list_of_files = []\n",
    "    ### open Files \n",
    "    for file in os.listdir(path):\n",
    "        count += 1\n",
    "    \n",
    "    list_dir_path = [x for x in os.listdir(path) if x.endswith(endswith)]    \n",
    "    \n",
    "    for Files in range(count):\n",
    "        list_of_files.append(path+delimiter_path+list_dir_path[Files])\n",
    "\n",
    "\n",
    "    return list_of_files\n",
    "\n",
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "\n",
    "\n",
    "def file_size(file_path):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \"\"\"\n",
    "    list_file_size = []\n",
    "    for i in range(len(file_path)):\n",
    "        if os.path.isfile(file_path[i]):\n",
    "            file_info = os.stat(file_path[i])\n",
    "\n",
    "            list_file_size.append(convert_bytes(file_info.st_size))\n",
    "\n",
    "    return list_file_size\n",
    "\n",
    "\n",
    "\n",
    "def MakeMetadataFile(file_path,file_size):\n",
    "    columns_list = [\"File_ID\",\"File_Path_Origin\",\"File_Size\",\"File_Date\"]\n",
    "    df = pd.DataFrame(columns = columns_list)\n",
    "    for i in range(len(file_path)):\n",
    "\n",
    "        df.loc[i] = [i+1,file_path[i],file_size[i],datetime.now()]\n",
    "\n",
    "    df.index = df.index + 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def MoveFileToLandingZone(source_path, endswith,linkedin_contains,glassdoor_soc_contains, glassdoor_avi_contains, landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path,logfiles_path,delimiter_path):\n",
    "\n",
    "    file_path = GetFilesPath(source_path,endswith,delimiter_path)\n",
    "    files_sizes = file_size(file_path)\n",
    "    general_metadata = MakeMetadataFile(file_path, files_sizes)\n",
    "    file_destination = []\n",
    "    file_destination_size = []\n",
    "\n",
    "    for file in general_metadata.File_Path_Origin:\n",
    "        if linkedin_contains in file:\n",
    "\n",
    "            file_destination.append(landing_zone_linkedin_emp_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            \n",
    "            shutil.copy(file, landing_zone_linkedin_emp_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            \n",
    "            file_destination_size.append(file_size([landing_zone_linkedin_emp_path +\n",
    "                        file.split(delimiter_path)[-1]]))\n",
    "\n",
    "        elif glassdoor_soc_contains in file:\n",
    "\n",
    "            file_destination.append(landing_zone_glassdoor_soc_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "\n",
    "\n",
    "            shutil.copy(file, landing_zone_glassdoor_soc_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            \n",
    "            file_destination_size.append(file_size([landing_zone_glassdoor_soc_path +\n",
    "                        file.split(delimiter_path)[-1]]))\n",
    "            \n",
    "        elif glassdoor_avi_contains in file:\n",
    "\n",
    "            file_destination.append(landing_zone_glassdoor_avi_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "\n",
    "            shutil.copy(file, landing_zone_glassdoor_avi_path +\n",
    "                        file.split(delimiter_path)[-1])\n",
    "            file_destination_size.append(file_size([landing_zone_glassdoor_avi_path +\n",
    "                        file.split(delimiter_path)[-1]]))\n",
    "            \n",
    "\n",
    "    general_metadata[\"File_Path_Destination\"] = file_destination\n",
    "    general_metadata[\"File_Destination_Size\"] = file_destination_size\n",
    "\n",
    "    general_metadata[\"File_Destination_Size\"] = general_metadata[\"File_Destination_Size\"].apply(lambda x: x[0])\n",
    "\n",
    "    general_metadata.to_csv(logfiles_path+\"/metadata-technical.csv\",index=None,encoding=\"utf-8\",header=True)\n",
    "\n",
    "\n",
    "    return general_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB\"\n",
    "logfiles_path = \"/home/virus/Documents/TD_DATALAKE/LOGFILES\"\n",
    "landing_zone_linkedin_emp_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/LINKEDIN/EMP/\"\n",
    "landing_zone_glassdoor_soc_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/SOC/\"\n",
    "landing_zone_glassdoor_avi_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/AVI/\"\n",
    "linkedin_contains = \"LINKEDIN\"\n",
    "glassdoor_soc_contains = \"INFO-SOC\"\n",
    "glassdoor_avi_contains = \"AVIS-SOC\"\n",
    "path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB\"\n",
    "endswith = \".html\"\n",
    "contains_1 = \"INFO\"\n",
    "contains_2 = \"LINKEDIN\"\n",
    "delimiter_path = GetOS()\n",
    "name_file = \"metadata-technical.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = MoveFileToLandingZone(source_path, endswith, linkedin_contains, glassdoor_soc_contains, glassdoor_avi_contains,\n",
    "                                landing_zone_linkedin_emp_path, landing_zone_glassdoor_soc_path, landing_zone_glassdoor_avi_path, logfiles_path, delimiter_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_ID</th>\n",
       "      <th>File_Path_Origin</th>\n",
       "      <th>File_Size</th>\n",
       "      <th>File_Date</th>\n",
       "      <th>File_Path_Destination</th>\n",
       "      <th>File_Destination_Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>320.6 KB</td>\n",
       "      <td>2023-02-27 19:28:44.628131</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>320.6 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>223.4 KB</td>\n",
       "      <td>2023-02-27 19:28:44.630157</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>223.4 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>550.1 KB</td>\n",
       "      <td>2023-02-27 19:28:44.634159</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>550.1 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>196.9 KB</td>\n",
       "      <td>2023-02-27 19:28:44.637487</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>196.9 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>326.9 KB</td>\n",
       "      <td>2023-02-27 19:28:44.640354</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>326.9 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>599</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>484.0 KB</td>\n",
       "      <td>2023-02-27 19:28:45.360059</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>484.0 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>600</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>233.2 KB</td>\n",
       "      <td>2023-02-27 19:28:45.361070</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>233.2 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>601</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>202.5 KB</td>\n",
       "      <td>2023-02-27 19:28:45.362049</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>202.5 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>602</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>195.6 KB</td>\n",
       "      <td>2023-02-27 19:28:45.363078</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>195.6 KB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>603</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>217.7 KB</td>\n",
       "      <td>2023-02-27 19:28:45.364077</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...</td>\n",
       "      <td>217.7 KB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>603 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_ID                                   File_Path_Origin File_Size  \\\n",
       "1          1  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  320.6 KB   \n",
       "2          2  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  223.4 KB   \n",
       "3          3  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  550.1 KB   \n",
       "4          4  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  196.9 KB   \n",
       "5          5  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  326.9 KB   \n",
       "..       ...                                                ...       ...   \n",
       "599      599  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  484.0 KB   \n",
       "600      600  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  233.2 KB   \n",
       "601      601  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  202.5 KB   \n",
       "602      602  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  195.6 KB   \n",
       "603      603  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  217.7 KB   \n",
       "\n",
       "                     File_Date  \\\n",
       "1   2023-02-27 19:28:44.628131   \n",
       "2   2023-02-27 19:28:44.630157   \n",
       "3   2023-02-27 19:28:44.634159   \n",
       "4   2023-02-27 19:28:44.637487   \n",
       "5   2023-02-27 19:28:44.640354   \n",
       "..                         ...   \n",
       "599 2023-02-27 19:28:45.360059   \n",
       "600 2023-02-27 19:28:45.361070   \n",
       "601 2023-02-27 19:28:45.362049   \n",
       "602 2023-02-27 19:28:45.363078   \n",
       "603 2023-02-27 19:28:45.364077   \n",
       "\n",
       "                                 File_Path_Destination File_Destination_Size  \n",
       "1    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              320.6 KB  \n",
       "2    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              223.4 KB  \n",
       "3    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              550.1 KB  \n",
       "4    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              196.9 KB  \n",
       "5    /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              326.9 KB  \n",
       "..                                                 ...                   ...  \n",
       "599  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              484.0 KB  \n",
       "600  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              233.2 KB  \n",
       "601  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              202.5 KB  \n",
       "602  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              195.6 KB  \n",
       "603  /home/virus/Documents/TD_DATALAKE/DATALAKE/1_L...              217.7 KB  \n",
       "\n",
       "[603 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadMetadataFiles(path,delimiter_path,name_file):\n",
    "    data = pd.read_csv(path+delimiter_path+name_file,encoding=\"utf-8\")\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/0_SOURCE_WEB\"\n",
    "landing_zone_linkedin_emp_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/LINKEDIN/EMP/\"\n",
    "file_path = GetFilesPath(source_path)\n",
    "files_sizes = file_size(file_path)\n",
    "\n",
    "linkedin_contains = \"LINKEDIN\"\n",
    "\n",
    "\n",
    "\n",
    "df = MakeMetadataFile(file_path,files_sizes)\n",
    "\n",
    "\n",
    "\n",
    "''' Function moove linkedin file to landing zone '''\n",
    "def MoveFileToLandingZone(source_path,linkedin_contains,landing_zone_linkedin_emp_path):\n",
    "    \n",
    "    #list_data = []\n",
    "    columns_dataframe = [\"title\",\"society\",\"city\"]\n",
    "    data_title = []\n",
    "    data_society = []\n",
    "    data_city = []\n",
    "    \n",
    "    file_path = GetFilesPath(source_path)\n",
    "    files_sizes = file_size(file_path)\n",
    "    df = MakeMetadataFile(file_path,files_sizes)\n",
    "\n",
    "\n",
    "    for file in df.File_Name:\n",
    "        if linkedin_contains in file:\n",
    "            \n",
    "            with open(file, 'r', encoding=\"utf-8\",errors=\"replace\") as f:\n",
    "                soup = bs(f, 'html.parser')\n",
    "                title = [i for i in soup.find_all('h1', attrs = {'class':'topcard__title'})]\n",
    "                society = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor'})]\n",
    "                city = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor topcard__flavor--bullet'})]\n",
    "                data_title.append(title[0].text)\n",
    "                data_society.append(society[0].text)\n",
    "                data_city.append(city[0].text)\n",
    "                \n",
    "                df_data = pd.DataFrame(columns = columns_dataframe)\n",
    "\n",
    "                df_data = pd.DataFrame(list(zip(data_title,data_society,data_city)),columns=columns_dataframe)\n",
    "\n",
    "            shutil.copy(file,landing_zone_linkedin_emp_path+file.split(\"/\")[-1])\n",
    "\n",
    "        \n",
    "    result = pd.concat([df,df_data],axis=1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "testing = MoveFileToLandingZone(source_path,linkedin_contains,landing_zone_linkedin_emp_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_ID</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>File_Size</th>\n",
       "      <th>File_Date</th>\n",
       "      <th>title</th>\n",
       "      <th>society</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>320.6 KB</td>\n",
       "      <td>2023-02-23 19:57:43.167425</td>\n",
       "      <td>Développeur Talend Big Data H/F</td>\n",
       "      <td>Accor</td>\n",
       "      <td>Évry, Île-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>223.4 KB</td>\n",
       "      <td>2023-02-23 19:57:43.168142</td>\n",
       "      <td>Consultant / Consultante décisionnel - Busines...</td>\n",
       "      <td>PERFECTSIGHT</td>\n",
       "      <td>Puteaux, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>550.1 KB</td>\n",
       "      <td>2023-02-23 19:57:43.169193</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Teads.tv</td>\n",
       "      <td>Paris, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>196.9 KB</td>\n",
       "      <td>2023-02-23 19:57:43.170375</td>\n",
       "      <td>CDI Ingénieur de Développement / moteurs d'opt...</td>\n",
       "      <td>GROUPE M6</td>\n",
       "      <td>Neuilly-sur-Seine, Île-de-France, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>326.9 KB</td>\n",
       "      <td>2023-02-23 19:57:43.171607</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Awalee Consulting</td>\n",
       "      <td>Région de Paris, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>600.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>233.2 KB</td>\n",
       "      <td>2023-02-23 19:57:43.795660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>601.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>202.5 KB</td>\n",
       "      <td>2023-02-23 19:57:43.796711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>602.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>195.6 KB</td>\n",
       "      <td>2023-02-23 19:57:43.797723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>603.0</td>\n",
       "      <td>/home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...</td>\n",
       "      <td>217.7 KB</td>\n",
       "      <td>2023-02-23 19:57:43.798879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>INGENIEUR EN DEVELOPPEMENT BIG DATA H/F</td>\n",
       "      <td>Altran</td>\n",
       "      <td>Sophia Antipolis, FR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>604 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     File_ID                                          File_Name File_Size  \\\n",
       "1        1.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  320.6 KB   \n",
       "2        2.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  223.4 KB   \n",
       "3        3.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  550.1 KB   \n",
       "4        4.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  196.9 KB   \n",
       "5        5.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  326.9 KB   \n",
       "..       ...                                                ...       ...   \n",
       "600    600.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  233.2 KB   \n",
       "601    601.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  202.5 KB   \n",
       "602    602.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  195.6 KB   \n",
       "603    603.0  /home/virus/Documents/TD_DATALAKE/DATALAKE/0_S...  217.7 KB   \n",
       "0        NaN                                                NaN       NaN   \n",
       "\n",
       "                     File_Date  \\\n",
       "1   2023-02-23 19:57:43.167425   \n",
       "2   2023-02-23 19:57:43.168142   \n",
       "3   2023-02-23 19:57:43.169193   \n",
       "4   2023-02-23 19:57:43.170375   \n",
       "5   2023-02-23 19:57:43.171607   \n",
       "..                         ...   \n",
       "600 2023-02-23 19:57:43.795660   \n",
       "601 2023-02-23 19:57:43.796711   \n",
       "602 2023-02-23 19:57:43.797723   \n",
       "603 2023-02-23 19:57:43.798879   \n",
       "0                          NaT   \n",
       "\n",
       "                                                 title            society  \\\n",
       "1                      Développeur Talend Big Data H/F              Accor   \n",
       "2    Consultant / Consultante décisionnel - Busines...       PERFECTSIGHT   \n",
       "3                                       Data Scientist           Teads.tv   \n",
       "4    CDI Ingénieur de Développement / moteurs d'opt...          GROUPE M6   \n",
       "5                                       Data Scientist  Awalee Consulting   \n",
       "..                                                 ...                ...   \n",
       "600                                                NaN                NaN   \n",
       "601                                                NaN                NaN   \n",
       "602                                                NaN                NaN   \n",
       "603                                                NaN                NaN   \n",
       "0              INGENIEUR EN DEVELOPPEMENT BIG DATA H/F             Altran   \n",
       "\n",
       "                                         city  \n",
       "1                 Évry, Île-de-France, France  \n",
       "2                                 Puteaux, FR  \n",
       "3                                   Paris, FR  \n",
       "4    Neuilly-sur-Seine, Île-de-France, France  \n",
       "5                     Région de Paris, France  \n",
       "..                                        ...  \n",
       "600                                       NaN  \n",
       "601                                       NaN  \n",
       "602                                       NaN  \n",
       "603                                       NaN  \n",
       "0                        Sophia Antipolis, FR  \n",
       "\n",
       "[604 rows x 7 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.pivot_table(index=[\"title\",\"society\",\"city\"],values=[\"File_Name\"],aggfunc=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['320.6 KB']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_size([\"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/SOC/13713-AVIS-SOC-GLASSDOOR-E3142186_P1.html\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1181346619.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[68], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    for i enumerate(list):\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "for i enumerate(list):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function to get data from html file where the file name contains a specific string (INFO) AND (LINKEDIN) '''\n",
    "def GetDataFromLinkedinFiles(opened_files):\n",
    "    list_of_data = []\n",
    "    columns_dataframe = [\"file\",\"title\",\"society\",\"city\"]\n",
    "    data_title = []\n",
    "    data_society = []\n",
    "    data_city = []\n",
    "    for i in range(len(opened_files)):\n",
    "        with open(opened_files[i], 'r', encoding=\"utf-8\",errors=\"replace\") as f:\n",
    "\n",
    "            soup = bs(f, 'html.parser')\n",
    "            title = [i for i in soup.find_all('h1', attrs = {'class':'topcard__title'})]\n",
    "            society = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor'})]\n",
    "            city = [i for i in soup.find_all('span', attrs = {'class':'topcard__flavor topcard__flavor--bullet'})]\n",
    "            data_title.append(title[0].text)\n",
    "            data_society.append(society[0].text)\n",
    "            data_city.append(city[0].text)\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(opened_files,data_title,data_society,data_city)),columns=columns_dataframe)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elif \"GLASSDOOR\" and \"AVI\" in i:\n",
    "        #    shutil.copy(i,\"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/AVI/\"+i.split(\"/\")[-1])\n",
    "        #elif \"SOC\" and \"GLASSDOOR\" in i:\n",
    "        #    shutil.copy(i,\"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/GLASSDOOR/SOC/\"+i.split(\"/\")[-1])\n",
    "        #else:\n",
    "        #    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/virus/Documents/TD_DATALAKE/LOGFILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glassdoor_emp_path,glassdoor_soc_path ,linkedin_contains,glassdoor_contains,glassdoor_soc_contains_1,glassdoor_soc_contains_2):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "    for i in range(len(file_path)):\n",
    "        if linkedin_contains in file_path[i] and endswith in file_path[i]:\n",
    "            shutil.copy(file_path[i],landing_zone_linkedin_emp_path+\"/\"+i)\n",
    "\n",
    "\n",
    "\n",
    "        elif contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],landing_zone_glassdoor_emp_path+\"/\"+file_path[i].split(\"\\\\\")[-1])\n",
    "        \n",
    "        elif if contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],glassdoor_soc_path+\"\\\\\"+file_path[i].split(\"\\\\\")[-1])\n",
    "\n",
    "\n",
    "def MoveGlassdoorAvisToLandingZone(file_path,glassdoor_emp_path,contains_1,contains_2):\n",
    "    for i in range(len(file_path)):\n",
    "        if contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],glassdoor_emp_path+\"\\\\\"+file_path[i].split(\"\\\\\")[-1])\n",
    "\n",
    "def MoveGlassdoorSocToLandingZone(file_path,glassdoor_soc_path,contains_1,contains_2):\n",
    "    for i in range(len(file_path)):\n",
    "        if contains_1 in file_path[i] and contains_2 in file_path[i]:\n",
    "            shutil.copy(file_path[i],glassdoor_soc_path+\"\\\\\"+file_path[i].split(\"\\\\\")[-1])\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#file_path = GetFilesPath(path)\n",
    "#print(file_path[0])\n",
    "#landing_zone_linkedin_emp_path = \"/home/virus/Documents/TD_DATALAKE/DATALAKE/1_LANDING_ZONE/LINKEDIN/EMP\"\n",
    "#landing_zone_glassdoor_emp_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\AVI\"\n",
    "#landing_zone_glassdoor_soc_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\SOC\"\n",
    "\n",
    "#endswith = \".html\"\n",
    "#linkedin_contains = \"LINKEDIN\"\n",
    "#glassdoor_contains = \"GLASSDOR\"\n",
    "#glassdoor_soc_contains_1 = \"SOC\"\n",
    "#glassdoor_soc_contains_2 = \"AVIS\"\n",
    "\n",
    "#MoveFileToLandingZone(file_path,linkedin_emp_path,glassdoor_emp_path,glassdoor_soc_path ,linkedin_contains,glassdoor_contains,glassdoor_soc_contains_1,glassdoor_soc_contains_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MoveFileToLandingZone(file_path,landing_zone_linkedin_emp_path,endswith,linkedin_contains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13546-AVIS-SOC-GLASSDOOR-E12966_P1.html'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = GetFilesPath(path)\n",
    "\n",
    "file_path[0].split(\"\\\\\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m file_path \u001b[39m=\u001b[39m GetFilesPath(path)\n\u001b[0;32m      2\u001b[0m linkedin_emp_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mTD_DATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m1_LANDING_ZONE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mLINKEDIN\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mEMP\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m glassdoor_emp_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mTD_DATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mDATALAKE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m1_LANDING_ZONE\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mGLASSDOOR\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mAVI\u001b[39m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[33], line 12\u001b[0m, in \u001b[0;36mGetFilesPath\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      9\u001b[0m list_dir_path \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(path) \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mendswith(endswith)]    \n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m Files \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(count):\n\u001b[1;32m---> 12\u001b[0m     list_of_files\u001b[39m.\u001b[39mappend(path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mlist_dir_path[Files])\n\u001b[0;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m list_of_files\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "file_path = GetFilesPath(path)\n",
    "linkedin_emp_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\LINKEDIN\\\\EMP\"\n",
    "glassdoor_emp_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\AVI\"\n",
    "glassdoor_soc_path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\1_LANDING_ZONE\\\\GLASSDOOR\\\\SOC\"\n",
    "\n",
    "endswith = \".html\"\n",
    "\n",
    "\n",
    "linkedin_contains = \"LINKEDIN\"\n",
    "glassdoor_contains = \"GLASSDOR\"\n",
    "glassdoor_soc_contains_1 = \"SOC\"\n",
    "glassdoor_soc_contains_2 = \"AVIS\"\n",
    "\n",
    "MoveLinkedinFileToLandingZone(file_path,linkedin_emp_path,linkedin_contains_1,linkedin_contains_2)\n",
    "\n",
    "MoveGlassdoorSocToLandingZone(file_path,glassdoor_emp_path,glassdoor_soc_contains_1,glassdoor_soc_contains_2)\n",
    "\n",
    "MoveGlassdoorAvisToLandingZone(file_path,glassdoor_soc_path,glassdoor_soc_contains_1,glassdoor_soc_contains_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function to count number of file in a directory '''\n",
    "def GetPathLinkedinFiles(path,endswith,info,linkedin):\n",
    "    count = 0\n",
    "    list_of_files = []\n",
    "    ### open Files \n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(endswith) and file.__contains__(info) and file.__contains__(linkedin):\n",
    "            count += 1\n",
    "    list_dir_path = [x for x in os.listdir(path) if x.endswith(endswith) and x.__contains__(info) and x.__contains__(linkedin)]    \n",
    "    for FileLinkedin in range(count):\n",
    "        list_of_files.append(path+\"\\\\\"+list_dir_path[FileLinkedin])\n",
    "\n",
    "    return list_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2 GB\n"
     ]
    }
   ],
   "source": [
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "\n",
    "\n",
    "def file_size(file_path):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \"\"\"\n",
    "    if os.path.isfile(file_path):\n",
    "        file_info = os.stat(file_path)\n",
    "        return convert_bytes(file_info.st_size)\n",
    "\n",
    "\n",
    "# Lets check the file size of MS Paint exe \n",
    "# or you can use any file path\n",
    "\n",
    "file_path = r\"C:\\\\Users\\\\virus\\\\Documents\\\\Win11_22H2_French_x64v1.iso\"\n",
    "\n",
    "#file_path = r\"C:\\\\Users\\\\virus\\\\Downloads\\\\PowerBIReportServer (1).exe\"\n",
    "#file_path = r\"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\\\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html\"\n",
    "print(file_size(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = \"INFO\"\n",
    "linkedin = \"LINKEDIN\"\n",
    "path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\"\n",
    "endswith = \".html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html\n",
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13799-INFO-EMP-LINKEDIN-FR-1555991658.html\n"
     ]
    }
   ],
   "source": [
    "all_path_linkedin_file = GetPathLinkedinFiles(path,endswith,info,linkedin)\n",
    "print(all_path_linkedin_file[0])\n",
    "print(all_path_linkedin_file[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>title</th>\n",
       "      <th>society</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INF...</td>\n",
       "      <td>Ingénieur Expert(e) Talend (H/F) / Freelance</td>\n",
       "      <td>Freelance-info.fr</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13547-INF...</td>\n",
       "      <td>INGÉNIEUR COMMERCIAL</td>\n",
       "      <td>Groupe PSIH</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13548-INF...</td>\n",
       "      <td>Ingénieur Expert(e) Talend F/H</td>\n",
       "      <td>CELAD</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13549-INF...</td>\n",
       "      <td>Ingénieur Expert(e) Talend (H/F)</td>\n",
       "      <td>CELAD</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13550-INF...</td>\n",
       "      <td>Consultant Big Data</td>\n",
       "      <td>Sopra Steria</td>\n",
       "      <td>Lyon, FR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file  \\\n",
       "0  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INF...   \n",
       "1  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13547-INF...   \n",
       "2  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13548-INF...   \n",
       "3  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13549-INF...   \n",
       "4  C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13550-INF...   \n",
       "\n",
       "                                          title            society      city  \n",
       "0  Ingénieur Expert(e) Talend (H/F) / Freelance  Freelance-info.fr  Lyon, FR  \n",
       "1                          INGÉNIEUR COMMERCIAL        Groupe PSIH  Lyon, FR  \n",
       "2                Ingénieur Expert(e) Talend F/H              CELAD  Lyon, FR  \n",
       "3              Ingénieur Expert(e) Talend (H/F)              CELAD  Lyon, FR  \n",
       "4                           Consultant Big Data       Sopra Steria  Lyon, FR  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = GetDataFromLinkedinFiles(all_path_linkedin_file)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"C:\\\\TD_DATALAKE\\DATALAKE\\\\1_LANDING_ZONE\\\\LINKEDIN\\EMP\\\\title.csv\",index=None,encoding=\"utf-8\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"C:\\\\TD_DATALAKE\\DATALAKE\\\\1_LANDING_ZONE\\\\LINKEDIN\\EMP\\\\title.csv\",index=True,encoding=\"utf-8\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\\\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = open(url,encoding=\"utf-8\",errors=\"replace\")\n",
    "soup = bs(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul class=\"job-criteria__list\">\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Niveau hiérarchique</h3><span class=\"job-criteria__text job-criteria__text--criteria\">Premier emploi</span>\n",
       "</li>\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Type d’emploi</h3><span class=\"job-criteria__text job-criteria__text--criteria\">Temps plein</span>\n",
       "</li>\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Fonction</h3><span class=\"job-criteria__text job-criteria__text--criteria\">Ingénierie</span><span class=\"job-criteria__text job-criteria__text--criteria\">Technologies de l’information</span>\n",
       "</li>\n",
       "<li class=\"job-criteria__item\">\n",
       "<h3 class=\"job-criteria__subheader\">Secteurs</h3><span class=\"job-criteria__text job-criteria__text--criteria\">Technologies et services de\n",
       "                            l’information</span><span class=\"job-criteria__text job-criteria__text--criteria\">Logiciels\n",
       "                            informatiques</span><span class=\"job-criteria__text job-criteria__text--criteria\">Recrutement </span>\n",
       "</li>\n",
       "</ul>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2 = soup.find(\"ul\",{'class':\"job-criteria__list\"})\n",
    "test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h3 class=\"job-criteria__subheader\">Niveau hiérarchique</h3>, <h3 class=\"job-criteria__subheader\">Type d’emploi</h3>, <h3 class=\"job-criteria__subheader\">Fonction</h3>, <h3 class=\"job-criteria__subheader\">Secteurs</h3>]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "## http://carrefax.com/new-blog/2018/9/5/find-child-elements-using-beautifulsoup\n",
    "\n",
    "test = test_2.find_all(\"h3\",{'class':'job-criteria__subheader'})\n",
    "print(test)\n",
    "\n",
    "for i in test:\n",
    "    children = i.findChildren('span',{'class':\"job-criteria__text job-criteria__text--criteria\"},recursive=True)\n",
    "    print(children)\n",
    "    for child in children:\n",
    "        print(child)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test.find_allChildren('span',{'class':\"job-criteria__text job-criteria__text--criteria\"},recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NavigableString' object has no attribute 'findChildren'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m test:\n\u001b[1;32m----> 3\u001b[0m     children \u001b[39m=\u001b[39m i\u001b[39m.\u001b[39;49mfindChildren(\u001b[39m\"\u001b[39m\u001b[39mspan\u001b[39m\u001b[39m\"\u001b[39m,{\u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mjob-criteria__text job-criteria__text--criteria\u001b[39m\u001b[39m\"\u001b[39m},recursive\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m children:\n\u001b[0;32m      5\u001b[0m         i_want \u001b[39m=\u001b[39m child\u001b[39m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\TD_DATALAKE\\venv\\Lib\\site-packages\\bs4\\element.py:965\u001b[0m, in \u001b[0;36mNavigableString.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n\u001b[0;32m    964\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 965\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    966\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[0;32m    967\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NavigableString' object has no attribute 'findChildren'"
     ]
    }
   ],
   "source": [
    "for i in test:\n",
    "\n",
    "    children = i.findChildren(\"span\",{'class':\"job-criteria__text job-criteria__text--criteria\"},recursive=True)\n",
    "    for child in children:\n",
    "        i_want = child.text\n",
    "        print(i_want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Niveau hiérarchique']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child = test.contents\n",
    "child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingénieur Expert(e) Talend (H/F) / Freelance\n"
     ]
    }
   ],
   "source": [
    "for title in soup.find_all('h1', attrs = {'class':'topcard__title'}): \n",
    "    print(title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Niveau hiérarchique\n",
      "Type d’emploi\n",
      "Fonction\n",
      "Secteurs\n"
     ]
    }
   ],
   "source": [
    "for i in soup.find_all('h3', attrs = {'class':'job-criteria__subheader'}): \n",
    "    print(i.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premier emploi\n",
      "Temps plein\n",
      "Ingénierie\n",
      "Technologies de l’information\n",
      "Technologies et services de\n",
      "                            l’information\n",
      "Logiciels\n",
      "                            informatiques\n",
      "Recrutement \n"
     ]
    }
   ],
   "source": [
    "for j in soup.find_all('span', attrs = {'class':'job-criteria__text job-criteria__text--criteria'}):\n",
    "    print(j.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issu du monde bancaire, CELAD a été créé en 1990\n",
      "                    à Toulouse. Nos 1350 collaborateurs(trices) interviennent aujourd’hui sur 2 pôles d’activités : les\n",
      "                    systèmes d’information et l’informatique industrielle et ceci chez plus de 250 clients (PME/PMI et\n",
      "                    grands comptes).Depuis 2006, l’entité CELAD Auvergne-Rhône-Alpes propose de nombreuses missions\n",
      "                    intéressantes et à forte valeur ajoutée sur l’ensemble de la région : Clermont-Ferrand, Montluçon,\n",
      "                    Lyon, Grenoble, St-Etienne, Valence, Dijon et Archamps.Nous sommes entièrement impliqué(e)s dans\n",
      "                    la gestion de nos collaborateurs(trices), ce qui est la base de notre développement ainsi que le\n",
      "                    reflet de la qualité de notre travail.Dotée d’un véritable processus de recrutement, l’agence\n",
      "                    CELAD Auvergne-Rhône-Alpes accompagne ses candidat(e)s avec beaucoup d’enthousiasme et de passion au\n",
      "                    quotidien.Donc si vous souhaitez faire partie de l’aventure Celadienne, rejoignez-nous\n",
      "                    !D’ailleurs, nous poursuivons notre développement et recrutons actuellement un(e) Ingénieur\n",
      "                    Expert(e) Talend pour intervenir chez un de nos clients.Contexte : Vous interviendrez sur divers\n",
      "                    sujets liés à la gestion des EDI du Domaine Décisionnel.Vos Principales Missions\n",
      "                            Quotidiennes\n",
      "\n",
      " La convergence sur une plate-forme technique unique de l’ensemble des flux EDI existants.\n",
      "                        \n",
      " La mise en uvre avec nos partenaires d’EDI disponibles et l’accompagnement de ce processus\n",
      "                            de bout en bout. Cet accompagnement intègre :\n",
      "la communication et l’explicitation de nos spécifications sur les formats de flux et sur les\n",
      "                    modalités techniques de communication sécurisée auprès des partenaires concernésla recette des\n",
      "                    flux produits par nos partenairesl’accompagnement de la mise en production et de l’intégration\n",
      "                    sous la plate-forme de surveillance et de communication.\n",
      "\n",
      " La participation potentielle à nos projets d’extension de l’offre de service EDI afin de\n",
      "                            couvrir de nouveaux besoins d’échange de données.\n",
      "Environnement Technique\n",
      "\n",
      " Expertise décisionnelle\n",
      " Expertise Talend\n",
      " Connaissance des métiers de l’Assurance serait un plus apprécié mais non indispensable\n",
      "\n",
      "Formation : Bac +5 et expérience de 5 ans minimum sur un poste similaire\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "for libelle_emp in soup.find_all('div', attrs = {\"description__text description__text--rich\"}) :\n",
    "    print(libelle_emp.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\n",
      "13546-INFO-EMP-LINKEDIN-FR-1599984246.html\n",
      "C:\\TD_DATALAKE\\DATALAKE\\0_SOURCE_WEB\\13546-INFO-EMP-LINKEDIN-FR-1599984246.html\n"
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\"\n",
    "list_dir_path = os.listdir(\"C:\\\\TD_DATALAKE\\\\DATALAKE\\\\0_SOURCE_WEB\")\n",
    "list_file = [x for x in list_dir_path if x.endswith(\".html\")]\n",
    "list_file_info = [x for x in list_dir_path if x.endswith(\".html\") and x.__contains__(\"INFO\")]\n",
    "\n",
    "print(str(path))\n",
    "\n",
    "print(list_file_info[0])\n",
    "\n",
    "url = (str(path)+\"\\\\\"+str(list_file_info[0]))\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freelance-info.fr recrute pour des postes de Ingénieur Expert(e) Talend (H/F) / Freelance (Lyon, FR) | LinkedIn\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('title').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingénieur Expert(e) Talend (H/F) / Freelance (France)RechercherOffres d’emploiRéseauIgnorerIgnorerIgnorerIgnorer\n"
     ]
    }
   ],
   "source": [
    "print(soup.find('section').get_text())\n",
    "\n",
    "#soup.prettify()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ba4306b892c702d24615d398cf592267eb03fb7bf2d599eb2ccdad9d12df570"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
